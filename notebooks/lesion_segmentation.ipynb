{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesion Segmentation\n",
    "\n",
    "This is the notebook to run both binary and multiclass lesion segmentation with UNET.\n",
    "\n",
    "The first part contains common imports:\n",
    "1. uNet model architecture\n",
    "2. Loss functions\n",
    "3. Train, Eval, Inference functions\n",
    "4. Common Dataset Imports (custom transforms, unnormalise, visualise dataset)\n",
    "\n",
    "The second part involves training binary and multiclass lesion segmentations seperately\n",
    "\n",
    "This notebook assumes the following project structure:\n",
    "```bash\n",
    "Root\n",
    "├── notebooks\n",
    "│   └── notebook1.ipynb\n",
    "└── input\n",
    "    └── IDRID\n",
    "        ├── 1. Original Images\n",
    "        │   ├── a. Training Set\n",
    "        │   └── b. Testing Set\n",
    "        └── 2. All Segmentation Groundtruths\n",
    "            ├── a. Training Set\n",
    "            └── b. Testing Set\n",
    "                ├── 1. Microaneurysms\n",
    "                ├── 2. Haemorrhages\n",
    "                ├── 3. Hard Exudates\n",
    "                ├── 4. Soft Exudates\n",
    "                └── 5. Optic Disc\n",
    "```\n",
    "\n",
    "If you do not have the dataset, you may download it from our Google Drive or from here:<br>\n",
    "https://ieee-dataport.org/open-access/indian-diabetic-retinopathy-image-dataset-idrid \n",
    "\n",
    "Please install the A. Segmentation.zip and move the dataset into this folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import pandas as pd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNET model Architecture\n",
    "\n",
    "UNET is made out of 3 main architectures\n",
    "1. Encoder block made out of convolutional blocks\n",
    "2. BottleNeck layer made out of 2 conv2d\n",
    "3. Decoder block made out of transpose convolutional blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(torch.nn.Module):\n",
    "  \"\"\"convolutional block for that UNET\"\"\"\n",
    "  def __init__(self, in_channels:int, out_channels:int):\n",
    "    super(conv_block, self).__init__()\n",
    "    self.conv1 = torch.nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "    self.bn1 = torch.nn.BatchNorm2d(out_channels)\n",
    "    self.conv2 = torch.nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "    self.bn2 = torch.nn.BatchNorm2d(out_channels)\n",
    "    self.relu = torch.nn.ReLU()\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    x = self.relu(self.bn1(self.conv1(inputs)))\n",
    "    x = self.relu(self.bn2(self.conv2(x)))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_block(torch.nn.Module):\n",
    "  \"\"\" \n",
    "  encoder block that includes convolutional block and maxpooling\n",
    "  returns both values before maxpool and after maxpool (for skip connections)\n",
    "  \"\"\" \n",
    "  def __init__(self, in_channels:int, out_channels:int):\n",
    "    super(encoder_block, self).__init__()\n",
    "    self.conv = conv_block(in_channels, out_channels)\n",
    "    self.maxpool = torch.nn.MaxPool2d((2,2))\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    x = self.conv(inputs)\n",
    "    p = self.maxpool(x)\n",
    "    return x, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder_block(torch.nn.Module):\n",
    "  \"\"\"\n",
    "  decoder block that upsamples images and takes in skip connections\n",
    "  \"\"\"\n",
    "  def __init__(self, in_channels:int, out_channels:int):\n",
    "    super(decoder_block, self).__init__()\n",
    "    self.upsample = torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "    self.conv = conv_block(out_channels+out_channels, out_channels)\n",
    "\n",
    "  def forward(self, inputs, skip_connections):\n",
    "    x = self.upsample(inputs)\n",
    "    x = torch.cat((x, skip_connections), 1)\n",
    "    return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class uNetModel(torch.nn.Module):\n",
    "  \"\"\"UNET architecture\"\"\"\n",
    "  def __init__(self, n_classes):\n",
    "    super(uNetModel, self).__init__()\n",
    "    #--------------------------\n",
    "    # Encoder\n",
    "    #--------------------------\n",
    "    self.encoder1 = encoder_block(3, 64)\n",
    "    self.encoder2 = encoder_block(64, 128)\n",
    "    self.encoder3 = encoder_block(128, 256)\n",
    "    self.encoder4 = encoder_block(256, 512)\n",
    "\n",
    "    #--------------------------\n",
    "    # Bottleneck\n",
    "    #--------------------------\n",
    "    self.bottleneck = conv_block(512, 1024)\n",
    "\n",
    "    #--------------------------\n",
    "    # Encoder\n",
    "    #--------------------------\n",
    "    self.decoder1 = decoder_block(1024, 512)\n",
    "    self.decoder2 = decoder_block(512, 256)\n",
    "    self.decoder3 = decoder_block(256, 128)\n",
    "    self.decoder4 = decoder_block(128, 64)\n",
    "\n",
    "    #--------------------------\n",
    "    # Classifier\n",
    "    #--------------------------\n",
    "    self.classifier = torch.nn.Conv2d(64, n_classes, 1)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    x1, p1 = self.encoder1(inputs)\n",
    "    x2, p2 = self.encoder2(p1)\n",
    "    x3, p3 = self.encoder3(p2)\n",
    "    x4, p4 = self.encoder4(p3)\n",
    "    b = self.bottleneck(p4)\n",
    "\n",
    "    d1 = self.decoder1(b, x4)\n",
    "    d2 = self.decoder2(d1, x3)\n",
    "    d3 = self.decoder3(d2, x2)\n",
    "    d4 = self.decoder4(d3, x1)\n",
    "\n",
    "    output = self.classifier(d4)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions\n",
    "\n",
    "Contains the following classes of different loss functions:\n",
    "1. DiceLoss\n",
    "2. FocalLoss\n",
    "3. TverskyLoss\n",
    "4. FocalTverskyLoss\n",
    "5. LogCoshDice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(n_classes:int, y:Tensor):\n",
    "    \"\"\"\n",
    "    Function that converts input into tensor with one hot encoding for each class\n",
    "    Eg. [1,2,0,1,1] -> [[0, 0, 1, 0, 0], [1, 0, 0, 1, 1], [0, 1, 0, 0, 0]]\n",
    "\n",
    "    Args:\n",
    "        n_classes (int): number of classes in y\n",
    "        y (Tensor): input to be split into one hot encoding\n",
    "\n",
    "    Returns:\n",
    "        tensor object of one hot encoding\n",
    "    \"\"\"\n",
    "    y = y.long() #convert to ints first (class values)\n",
    "    batch_size, _, height, width = y.size()\n",
    "    one_hot = torch.zeros(batch_size, n_classes, height, width, device=y.device) #create base case\n",
    "    return one_hot.scatter_(1, y, 1)\n",
    "\n",
    "\n",
    "class DiceLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Function to calculate Dice Loss\n",
    "        DL(y,p) = 1 - 2(yp + e) / (y + p + e) \n",
    "    \n",
    "    where y = y_true, p = y_pred, e = epsilon to ensure no divisible by 0\n",
    "\n",
    "    Args:\n",
    "        reduction (str, optional): Reduction method, either only mean or sum. Defaults to mean\n",
    "        eps (float, optional): Value to avoid undefined edge cases. Defaults to 1e-6.\n",
    "\n",
    "    Returns:\n",
    "        dice loss\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction:str=\"mean\", eps:float=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.epsilon = eps\n",
    "        \n",
    "        if reduction in [\"mean\", \"sum\"]:\n",
    "            self.reduction = reduction\n",
    "        else:\n",
    "            raise Exception(\"Invalid Reduction Method Selected. Please choose between mean and sum.\")\n",
    "    \n",
    "    \n",
    "    def forward(self, y_pred:Tensor, y_true:Tensor):\n",
    "        \"\"\"\n",
    "        y_pred (Tensor): model prediction\n",
    "        y_true (Tensor): true value\n",
    "        \"\"\"\n",
    "        #get num of classes\n",
    "        _, n_classes, _, _  = y_pred.size()\n",
    "\n",
    "        if n_classes == 1: #binary classification, of 1 or 0s, sigmoid\n",
    "            y_pred = torch.nn.functional.sigmoid(y_pred)\n",
    "\n",
    "        if n_classes > 1: #multiclass classification\n",
    "            y_pred = torch.nn.functional.softmax(y_pred, dim=1)\n",
    "            y_true =  one_hot_encode(n_classes, y_true)\n",
    "\n",
    "        assert y_true.size() == y_pred.size() #checking pred and true has the same size\n",
    "\n",
    "        intersection = (y_pred * y_true).sum(dim=(2, 3)) #yp\n",
    "        union = y_pred.sum(dim=(2,3)) + y_true.sum(dim=(2,3)) #y+p\n",
    "\n",
    "        soft_dice_loss = (2.0* (intersection + self.epsilon)) / (union + self.epsilon)\n",
    "        \n",
    "        if self.reduction  == \"mean\":\n",
    "            return 1.0 - torch.mean(soft_dice_loss)\n",
    "        \n",
    "        else:\n",
    "            return 1.0 - torch.sum(soft_dice_loss)\n",
    "\n",
    "\n",
    "\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Function to calculate Focal Loss\n",
    "        FL(p, y) = -alpha * ((1-pt) ** gamma) * log(pt) \n",
    "    where log(pt) is the cross entropy loss, p = prediction, y = true value\n",
    "\n",
    "    Args:\n",
    "        n_classes (int): Num of classes\n",
    "        alpha (float, optional): alpha value, adds weights to help with class imbalance. Defaults to 0.25.\n",
    "        gamma (float, optional): gamma value, controls the shape of loss curve for pioritising. Defaults to 2.0.\n",
    "        reduction (str, optional): Reduction method, either none, mean or sum. Defaults to \"mean\".\n",
    "    \"\"\"\n",
    "    def __init__(self, weights:Tensor=None, alpha:float=0.75, gamma:float=2.0, reduction:str=\"mean\"):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.weights = weights\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if reduction in [\"none\", \"mean\", \"sum\"]:\n",
    "            self.reduction = reduction\n",
    "        else:\n",
    "            raise Exception(\"Invalid Reduction Method Selected. Please choose between none, mean and sum.\")\n",
    "    \n",
    "    \n",
    "    def forward(self, y_pred:Tensor, y_true:Tensor):\n",
    "        \"\"\"\n",
    "        y_pred (Tensor): model prediction\n",
    "        y_true (Tensor): true value\n",
    "        \"\"\"\n",
    "        _, n_classes, _, _  = y_pred.size()\n",
    "\n",
    "        if n_classes == 1: #binary classification, of 1 or 0s, sigmoid\n",
    "            loss = torch.nn.functional.binary_cross_entropy_with_logits(y_pred, y_true.float(), pos_weight=self.weights, reduction=self.reduction) #-log(pt)\n",
    "\n",
    "        if n_classes > 1: #multiclass classification\n",
    "            y_true = torch.squeeze(y_true.long(), dim=1) #squeeze class layer as cross entropy expect (batch_size, h, w)\n",
    "            loss = torch.nn.functional.cross_entropy(y_pred, y_true, weight=self.weights,  reduction=self.reduction) #-log(pt)\n",
    "\n",
    "        exp_loss = torch.exp(-loss) #pt\n",
    "        focal_loss = self.alpha * torch.pow((1.0-exp_loss), self.gamma) * loss\n",
    "        #reduction already done in the loss calculation, just return focal loss\n",
    "\n",
    "        return focal_loss\n",
    "    \n",
    "\n",
    "\n",
    "class TverskyLoss(torch.nn.Module):\n",
    "    def __init__(self, beta:float=0.5, eps:float=1e-6):\n",
    "        \"\"\"\n",
    "        Function to calculate TverskyLoss\n",
    "            TI(y, p) = yp / yp + beta(1-p)y + (1-beta)(1-y)p\n",
    "            TverskyLoss = 1- TI(y,p)\n",
    "        where y = true value and p = prediction\n",
    "\n",
    "        Args:\n",
    "            beta (float, optional): Beta value, adds weight to false negative and false positive. Defaults to 0.5.\n",
    "            eps (float, optional): Value to avoid undefined edge cases. Defaults to 1e-6.\n",
    "        \"\"\"\n",
    "        super(TverskyLoss, self).__init__()\n",
    "        self.beta = beta\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, y_pred:Tensor, y_true:Tensor):\n",
    "        \"\"\"\n",
    "        y_pred (Tensor): model prediction\n",
    "        y_true (Tensor): true value\n",
    "        \"\"\"\n",
    "        _, n_classes, _, _  = y_pred.size()\n",
    "\n",
    "        if n_classes == 1: #binary classification, of 1 or 0s, sigmoid\n",
    "            y_pred = torch.nn.functional.sigmoid(y_pred)\n",
    "\n",
    "        if n_classes > 1: #multiclass classification\n",
    "            y_pred = torch.nn.functional.softmax(y_pred, dim=1)\n",
    "            y_true =  one_hot_encode(n_classes, y_true)\n",
    "\n",
    "        true_positive = (y_pred * y_true).sum(dim=(2, 3)) #yp\n",
    "        false_negative =  ((1-y_pred) * y_true).sum(dim=(2,3)) #(1-p)y\n",
    "        false_positive = (y_pred * (1-y_true)).sum(dim=(2,3)) #(1-y)p\n",
    "\n",
    "        tversky_loss = true_positive / (true_positive + self.beta*false_positive + (1-self.beta)*false_negative + self.eps)\n",
    "        return 1- torch.mean(tversky_loss)\n",
    "    \n",
    "\n",
    "\n",
    "class FocalTverskyLoss(torch.nn.Module):\n",
    "    def __init__(self, beta:float=0.5, gamma:float=1.33, eps:float=1e-6):\n",
    "        \"\"\"\n",
    "        Function to calculate FocalTverskyLoss\n",
    "            FTL = TverskyLoss ** gamma\n",
    "\n",
    "        Args:\n",
    "            beta (float, optional): Beta value, adds weight to false negative and false positive. Defaults to 0.5.\n",
    "            gamma (float, optional): Gamma value, ranges from [1,3]. Defaults to 1.33.\n",
    "            eps (float, optional): Value to avoid undefined edge cases. Defaults to 1e-6.\n",
    "        \"\"\"\n",
    "        super(FocalTverskyLoss, self).__init__()\n",
    "        self.treversky = TverskyLoss(beta, eps)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, y_pred:Tensor, y_true:Tensor):\n",
    "        \"\"\"\n",
    "        y_pred (Tensor): model prediction\n",
    "        y_true (Tensor): true value\n",
    "        \"\"\"\n",
    "        tversky_loss = self.treversky(y_pred, y_true)\n",
    "        focal_tversky_loss = torch.pow(tversky_loss, self.gamma)\n",
    "        return focal_tversky_loss\n",
    "\n",
    "\n",
    "class LogCoshDice(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Function to calculate LogCoshDice\n",
    "        LogCoshDice = log(cosh(diceloss))\n",
    "    \n",
    "    where cosh(x) = (e**x - e**-x) /2\n",
    "\n",
    "    Args:\n",
    "        reduction (str, optional): Reduction method, either only mean or sum. Defaults to mean\n",
    "        eps (float, optional): Value to avoid undefined edge cases. Defaults to 1e-6.\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction:str=\"mean\", eps:float=1e-6):\n",
    "        super(LogCoshDice, self).__init__()\n",
    "        self.diceloss = DiceLoss(reduction, eps)\n",
    "\n",
    "    def forward(self, y_pred:Tensor, y_true:Tensor):\n",
    "        \"\"\"\n",
    "        y_pred (Tensor): model prediction\n",
    "        y_true (Tensor): true value\n",
    "        \"\"\"\n",
    "        dice_loss = self.diceloss(y_pred, y_true)\n",
    "        return torch.log(torch.cosh(dice_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Eval, Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(n_classes:int, dataloader):\n",
    "    \"\"\"\n",
    "    Function to calculate the class frequencies and return the weights\n",
    "\n",
    "    Pos Weight (n_classes ==2 ): Num(negative labels) / Num(Positive Labels)\n",
    "    Class Weights (n_classes >2): sum(class_frequencies) / Num(class_labels)\n",
    "    \n",
    "    Args:\n",
    "        n_classes (int): number of classes\n",
    "        dataloader (Dataloader): dataloader to calculate the class frequencies from\n",
    "\n",
    "    Returns:\n",
    "        class_frequencies (dict): dictionary of class index and its frequency\n",
    "        pos_weight (float): Ratio of negative labels / positive labels. Returned only if n_classes = 2\n",
    "        class_weights (dict): Dictionary of class index and its respective weights. Returned only if n_classes > 2\n",
    "    \"\"\"\n",
    "\n",
    "    class_frequencies = np.zeros(n_classes)\n",
    "    for idx, (image, mask) in enumerate(dataloader):\n",
    "        #get class frequencies by iterating through each data\n",
    "        class_frequencies += np.bincount(mask.flatten(), minlength=n_classes)\n",
    "\n",
    "    if n_classes == 2:\n",
    "        pos_weight = class_frequencies[0]/class_frequencies[1]\n",
    "        return class_frequencies, pos_weight\n",
    "\n",
    "    elif n_classes > 2:\n",
    "        total_labels = np.sum(class_frequencies)\n",
    "        class_weights = total_labels/class_frequencies\n",
    "        return class_frequencies, class_weights\n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"Number of classes cannot be smaller than 2!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unet(model:uNetModel, criterion, optimiser, dataloader, num_epochs:int, device:str, folder_name:str, model_name:str=None):\n",
    "    \"\"\"\n",
    "    Function to train unet, returns loss per epoch    \n",
    "    Model with lowest loss is saved as models/{folder_name}/{model_name}.pt\n",
    "    Model after last epoch is saved as models/{folder_name}/{model_name}_{epoch_num}.pt\n",
    "\n",
    "    Args:\n",
    "        model (uNetModel): unet model to be trained\n",
    "        criterion: loss criterion function\n",
    "        optimiser: optimiser function\n",
    "        dataloade: train dataloader\n",
    "        num_epochs (int): number of training epoch\n",
    "        device (str): cuda or cpu\n",
    "        folder_name (str): folder name for model to be saved\n",
    "        model_name (str, optional): model name for model to be saved, if None no saving of models. Defaults to None\n",
    "\n",
    "    Returns:\n",
    "        loss_container (list): list of loss of each epoch\n",
    "    \"\"\"\n",
    "    #initiatlising folders\n",
    "    if not os.path.isdir(f\"../models/{folder_name}\"):\n",
    "        os.makedirs(f\"../models/{folder_name}\")\n",
    "    \n",
    "    #intialising training\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    loss_container = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for idx, (image, mask) in enumerate(tqdm(dataloader)):\n",
    "            (image, mask) = (image.to(device), mask.to(device))\n",
    "            outputs = model(image)\n",
    "\n",
    "            #For specific torch built-in losses (not losses that we manually made), convert the mask to the correct way to calculate the loss\n",
    "            if isinstance(criterion, torch.nn.BCEWithLogitsLoss):\n",
    "                loss = criterion(outputs, mask.float())\n",
    "            \n",
    "            elif isinstance(criterion, torch.nn.CrossEntropyLoss):\n",
    "                loss = criterion(outputs, torch.squeeze(mask, dim=1))\n",
    "            \n",
    "            else:\n",
    "                loss = criterion(outputs, mask)\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "        #best model\n",
    "        if len(loss_container) > 0  and avg_loss < min(loss_container) and model_name != None:\n",
    "            torch.save(model, f\"../models/{folder_name}/{model_name}.pt\")\n",
    "        \n",
    "        loss_container.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1} completed, training loss: {avg_loss:.2f}\")\n",
    "        \n",
    "    #last model\n",
    "    if model_name != None:\n",
    "        torch.save(model, f\"../models/{folder_name}/{model_name}_{num_epochs}.pt\")\n",
    "\n",
    "    return loss_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(preds:Tensor, labels:Tensor, multiclass:bool):\n",
    "    \"\"\"\n",
    "    Function to calculate dice coefficient\n",
    "    \n",
    "    Args:\n",
    "        preds (Tensor): Predicted tensor.\n",
    "        labels (Tensor): Ground truth tensor.\n",
    "    \n",
    "    Returns:\n",
    "        float: Dice Coefficient.\n",
    "    \"\"\"    \n",
    "\n",
    "    n_classes = preds.size()[1]\n",
    "    dice = []\n",
    "\n",
    "    if multiclass:\n",
    "        preds = torch.argmax(preds, dim=1, keepdim=True)\n",
    "        #label size = [batch_size, num_class, h, w]\n",
    "        labels =  one_hot_encode(n_classes, labels)\n",
    "        preds = one_hot_encode(n_classes, preds)\n",
    "\n",
    "    else:\n",
    "        preds = (preds > 0.5).float()\n",
    "\n",
    "    for i in range(n_classes):    \n",
    "        pred_f = preds[:, i, :, :].flatten()\n",
    "        labels_f = labels[:, i, :, :].flatten()\n",
    "\n",
    "        # Calculate Intersection and twice the sum of both predictions and labels\n",
    "        intersection = (pred_f * labels_f).sum().item()\n",
    "        total = (pred_f + labels_f).sum().item()\n",
    "\n",
    "        dice.append((2. * intersection) / total)\n",
    "    \n",
    "    return sum(dice) / len(dice)\n",
    "\n",
    "\n",
    "def jaccard_index(preds:Tensor, labels:Tensor, multiclass:bool):\n",
    "    \"\"\"\n",
    "    Function to calculate jaccard_index, IOU\n",
    "    Args:\n",
    "        preds (Tensor): Predicted tensor.\n",
    "        labels (Tensor): Ground truth tensor.\n",
    "    \n",
    "    Returns:\n",
    "        float: Jaccard Index.\n",
    "    \"\"\"    \n",
    "    n_classes = preds.size()[1]\n",
    "    jaccard = []\n",
    "\n",
    "    if multiclass:\n",
    "        preds = torch.argmax(preds, dim=1, keepdim=True)\n",
    "        #label size = [batch_size, num_class, h, w]\n",
    "        labels =  one_hot_encode(n_classes, labels)\n",
    "        preds = one_hot_encode(n_classes, preds)\n",
    "\n",
    "    else:\n",
    "        preds = (preds > 0.5).float()\n",
    "\n",
    "    for i in range(n_classes):    \n",
    "            pred_f = preds[:, i, :, :].flatten()\n",
    "            labels_f = labels[:, i, :, :].flatten()\n",
    "\n",
    "            # Calculate Intersection and twice the sum of both predictions and labels\n",
    "            intersection = (pred_f * labels_f).sum().item()\n",
    "            total = (pred_f + labels_f).sum().item()\n",
    "            union = total - intersection\n",
    "\n",
    "            jaccard.append( intersection/union )\n",
    "\n",
    "    return sum(jaccard) / len(jaccard)\n",
    "\n",
    "def eval_unet(model:uNetModel, multiclass:bool,  dataloader, device:str):\n",
    "    \"\"\"\n",
    "    Function to evaluate UNET inference using dice coefficient and jaccard index\n",
    "\n",
    "    Args:\n",
    "        model (uNetModel): unet model for evalutation\n",
    "        multiclass (bool): segmentation problem a binary or multiclass\n",
    "        dataloader: evaluation dataloader\n",
    "        device (str): cuda or cpu\n",
    "    \"\"\"\n",
    "    model.eval() \n",
    "    jaccard_scores = []\n",
    "    dice_scores = []\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for idx, (images, masks) in enumerate(dataloader):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            if multiclass:\n",
    "                outputs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "\n",
    "            else:    \n",
    "                outputs = torch.nn.functional.sigmoid(outputs)\n",
    "\n",
    "            # Calculate metrics for each batch and append to list\n",
    "            jaccard_scores.append(jaccard_index(outputs, masks, multiclass))\n",
    "            dice_scores.append(dice_coefficient(outputs, masks, multiclass))\n",
    "\n",
    "    # Compute average scores\n",
    "    avg_jaccard = sum(jaccard_scores) / len(jaccard_scores)\n",
    "    avg_dice = sum(dice_scores) / len(dice_scores)\n",
    "\n",
    "    print(f\"Average Dice Coefficient: {avg_dice}\")\n",
    "    print(f\"Average Jaccard Index: {avg_jaccard}\")\n",
    "\n",
    "    return avg_dice, avg_jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_unet(model:uNetModel, multiclass:bool, dataloader, num_images:int, device:str, show_plot:bool=True, mean:list=[0.2816, 0.2817, 0.2816], std:list=[0.1992, 0.1991, 0.1991]):\n",
    "    \"\"\"\n",
    "    Function to predict mask based on image input\n",
    "\n",
    "    Args:\n",
    "        model (uNetModel): trained uNet model for inference\n",
    "        multiclass (bool): segmentation problem a binary or multiclass\n",
    "        dataloader: dataloader to run inference on\n",
    "        num_images (int): number of images to run inference on\n",
    "        device (str): cuda or cpu\n",
    "        show_plot (bool): show results in a plot or just return image. Defaults to True.\n",
    "        mean (list, optional): mean value used in dataset. Defaults to [0.2816, 0.2817, 0.2816].\n",
    "        std (list, optional): std value used in dataset. Defaults to [0.1992, 0.1991, 0.1991].\n",
    "    \n",
    "    Returns:\n",
    "        results (list): List of model predictions\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    image_index = 0\n",
    "    image_per_batch = math.ceil(num_images/len(dataloader))\n",
    "    with torch.no_grad():\n",
    "        for idx in range(min(len(dataloader), num_images)):\n",
    "            image, mask = next(iter(dataloader))\n",
    "            outputs = model(image.to(device)) #run inference\n",
    "\n",
    "            #generate output based on multiclass or binary problem\n",
    "            if multiclass:\n",
    "                outputs = torch.nn.functional.softmax(outputs)\n",
    "                predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            else:    \n",
    "                outputs = torch.nn.functional.sigmoid(outputs)\n",
    "                predictions = (outputs > 0.5).float()\n",
    "\n",
    "\n",
    "            for i in range(image_per_batch):\n",
    "                results.append(predictions[i].cpu())\n",
    "\n",
    "                #if plotting graph\n",
    "                if show_plot:\n",
    "                    image = unnormalize(image[i], mean, std)\n",
    "                    image_np = np.array(image).transpose(1, 2, 0)\n",
    "                    pred_np = np.array(predictions[i].cpu())\n",
    "                    \n",
    "                    fig, axes = plt.subplots(1, 3, figsize=(10, 5))\n",
    "                    axes[0].imshow(image_np)  # Assuming batch size is 1\n",
    "                    axes[0].set_title('Input')\n",
    "                    axes[0].axis('off')\n",
    "                    axes[1].imshow(pred_np.squeeze(), cmap=\"gray\")  # Assuming batch size is 1\n",
    "                    axes[1].set_title('UNET Prediction')\n",
    "                    axes[1].axis('off')\n",
    "                    axes[2].imshow(np.array(mask[i]).squeeze(), cmap='gray')\n",
    "                    axes[2].set_title('True Value')\n",
    "                    axes[2].axis('off')\n",
    "                    plt.show()\n",
    "\n",
    "                image_index += 1\n",
    "                if image_index == num_images:\n",
    "                    break\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preprocessing & Visualisation\n",
    "\n",
    "Common Functions used in both binary and multiclass segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_contrast(image:Image, show_image:bool):    \n",
    "    \"\"\"\n",
    "    Function to enhance the contrast of the GREEN channel of a RGB image with CLAHE\n",
    "\n",
    "    Args:\n",
    "        image (Image): Image\n",
    "        show_image (bool): Boolean to show the enhanced image \n",
    "\n",
    "    Returns:\n",
    "        enhanced_image (Image): enhanced image \n",
    "    \"\"\"\n",
    "    image = np.array(image)\n",
    "    r,g,b = cv2.split(image) #extract out green channel\n",
    "\n",
    "    green_channel = cv2.cvtColor(g, cv2.COLOR_GRAY2RGB)  # convert green channel to RGB\n",
    "    lab = cv2.cvtColor(green_channel, cv2.COLOR_RGB2LAB)  # convert rgb image to lab channels\n",
    "\n",
    "    L, a, b = cv2.split(lab)  # split lab channels\n",
    "\n",
    "    clahe = cv2.createCLAHE(clipLimit=3, tileGridSize=(8, 8))  # create CLAHE\n",
    "    cL = clahe.apply(L)  # apply CLAHE to enhance contrast\n",
    "\n",
    "    new_img = cv2.merge((cL, a, b))  # create new lab enhanced image\n",
    "    enhanced_img = cv2.cvtColor(new_img, cv2.COLOR_LAB2RGB) # convert from lab to rgb\n",
    "\n",
    "\n",
    "    if show_image:\n",
    "        resized_enhanced_img = cv2.resize(enhanced_img, (224,224))\n",
    "        cv2.imshow(\"\", resized_enhanced_img)\n",
    "        cv2.waitKey(10000)\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    return Image.fromarray(enhanced_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensorWithoutScaling(object):\n",
    "    def __call__(self, mask):\n",
    "        \"\"\"|\n",
    "        Convert a PIL Image or numpy.ndarray to tensor without normalizing.\n",
    "        \"\"\"\n",
    "        return (transforms.functional.to_tensor(mask) * 255).to(torch.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize(image:np.array, mean:np.array, std:np.array):\n",
    "    \"\"\"\n",
    "    Function to unnormalize an image given its mean and std\n",
    "\n",
    "    Args:\n",
    "        image (np.array): Numpy array of the image\n",
    "        mean (np.array): Numpy array of the mean\n",
    "        std (np.array): Numpy array of the std\n",
    "\n",
    "    Returns:\n",
    "        Unnormalised numpy array of the image\n",
    "    \"\"\"\n",
    "\n",
    "    for t, m, s in zip(image, mean, std):\n",
    "        t.mul_(s).add_(m)    # for inplace operations\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "def visualise_dataset(dataloader, num_images=9, mean=[0.2816, 0.2817, 0.2816], std=[0.1992, 0.1991, 0.1991]):\n",
    "    \"\"\"\n",
    "    Function to show n_images of the dataset\n",
    "\n",
    "    Args:\n",
    "        dataloader: dataloader\n",
    "        num_images (int, optional): number of images to show. Defaults to 9.\n",
    "        mean (list, optional): mean of the dataloader images that were used to normalise. Defaults to [0.2816, 0.2817, 0.2816].\n",
    "        std (list, optional): std of the dataloader images that were used to normalise. Defaults to [0.1992, 0.1991, 0.1991].\n",
    "    \"\"\"\n",
    "    images, masks = next(iter(dataloader))\n",
    "    print(f\"Image batch shape: {images.size()}\")\n",
    "    print(f\"Masks batch shape: {masks.size()}\")\n",
    "\n",
    "    fig, axs = plt.subplots(num_images, 2, figsize=(8, 12), squeeze=False)\n",
    "\n",
    "    for i in range(num_images):\n",
    "        img = images[i].squeeze()\n",
    "        img = unnormalize(img, mean, std)  # Unnormalize the image\n",
    "        img = np.transpose(img.numpy(), (1, 2, 0))\n",
    "\n",
    "        mask = masks[i].squeeze()\n",
    "        mask = mask.numpy()\n",
    "\n",
    "        axs[i, 0].imshow(img)\n",
    "        axs[i, 0].axis('off')\n",
    "        axs[i, 0].set_title('Image')\n",
    "\n",
    "        axs[i, 1].imshow(mask.squeeze(), cmap='gray')\n",
    "        axs[i, 1].axis('off')\n",
    "        axs[i, 1].set_title('Mask')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Image Segmenetation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinarySegmentationDataset(Dataset):\n",
    "    def __init__(self, image_paths:str, mask_paths:str, mask_class:list, transform, masktransform, mean:list, std:list):\n",
    "        \"\"\"\n",
    "        Function to convert image folder to BINARY dataset\n",
    "\n",
    "        Args:\n",
    "            image_paths (str): string path to image folder\n",
    "            mask_paths (str): string path to mask folder\n",
    "            mask_class (list): list containing all the mask classes in the format of [[class 1 folder name, class 1 image annotation], [], ...]\n",
    "            transform: pyTorch Image transforms\n",
    "            masktransform: pyTorch Mask transforms\n",
    "            mean (list): mean to normalise image\n",
    "            std (list): std to normalise image\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.mask_class = mask_class        \n",
    "        self.transform = transform\n",
    "        self.masktransform = masktransform\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        \n",
    "        self.image_list = os.listdir(image_paths) #all image names\n",
    "        self.mask_list = []\n",
    "        for dir in mask_class:\n",
    "            self.mask_list.append(os.listdir(f\"{mask_paths}/{dir[0]}\")) #all mask names for each class \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_list[idx]\n",
    "        image_path = f\"{self.image_paths}/{image_name}\"\n",
    "        image = Image.open(image_path) #read image\n",
    "        enhanced_image = enhance_contrast(image, False) #enhance contrast of image\n",
    "        merged_mask = np.zeros((image.size[1],image.size[0]), dtype=np.uint8) #create base mask\n",
    "\n",
    "        for class_idx, mask_dir in enumerate(self.mask_class):\n",
    "            mask_name = f\"{image_name[:-4]}_{mask_dir[1]}.tif\" #create mask name which is image_name_{class annotation}.tif\n",
    "            if mask_name in self.mask_list[class_idx]: #if inside the folder (exist)\n",
    "                mask = Image.open(f\"{self.mask_paths}/{mask_dir[0]}/{mask_name}\").convert('L') #read and convert to greyscale\n",
    "                mask_array = np.array(mask)\n",
    "                merged_mask[mask_array > 0] = 1 #obtain mask and convert them to fixed class of 1 \n",
    "\n",
    "            else:\n",
    "                raise Exception(\"Mask image not found\")\n",
    "            \n",
    "        mask = Image.fromarray(merged_mask) #create mask image\n",
    "\n",
    "  \n",
    "        seed = np.random.randint(2147483647) # make a seed with numpy generator\n",
    "\n",
    "        if self.transform is not None:\n",
    "            random.seed(seed) # apply this seed to img transforms\n",
    "            torch.manual_seed(seed)\n",
    "            enhanced_image = self.transform(enhanced_image)\n",
    "            random.seed(seed) # apply this seed to mask transforms\n",
    "            torch.manual_seed(seed)\n",
    "            mask = self.masktransform(mask)\n",
    "\n",
    "\n",
    "        #normalising image\n",
    "        mean = torch.tensor(self.mean).reshape(-1, 1, 1)\n",
    "        std = torch.tensor(self.std).reshape(-1, 1, 1)\n",
    "\n",
    "        enhanced_image = ( enhanced_image - mean )/std #normalise image\n",
    "\n",
    "        return enhanced_image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameters\n",
    "image_size = (512,512)\n",
    "batch_size = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    ToTensorWithoutScaling()\n",
    "])\n",
    "\n",
    "aug_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomVerticalFlip(0.5),\n",
    "    transforms.RandomRotation((-45,45)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "mask_aug_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomVerticalFlip(0.5),\n",
    "    transforms.RandomRotation((-45,45)),\n",
    "    ToTensorWithoutScaling()\n",
    "])\n",
    "\n",
    "train_mask_file = \"../input/IDRID/A.%20Segmentation/A. Segmentation/2. All Segmentation Groundtruths/a. Training Set\"\n",
    "test_mask_file = \"../input/IDRID/A.%20Segmentation/A. Segmentation/2. All Segmentation Groundtruths/b. Testing Set\"\n",
    "class_path = [[\"1. Microaneurysms\",\"MA\"], [\"2. Haemorrhages\",\"HE\"], [\"3. Hard Exudates\",\"EX\"], [\"4. Soft Exudates\",\"SE\"], [\"5. Optic Disc\", \"OD\"]]\n",
    "\n",
    "# Initialize your dataset - 1/3 unagumented and 2/3 augmented\n",
    "train_dataset = BinarySegmentationDataset(image_paths = '../input/IDRID/A.%20Segmentation/A. Segmentation/1. Original Images/a. Training Set',\n",
    "                              mask_paths = train_mask_file,\n",
    "                              mask_class=class_path,\n",
    "                              transform=transform,\n",
    "                              masktransform=mask_transform,\n",
    "                              mean=[0.2816, 0.2817, 0.2816],\n",
    "                              std=[0.1992, 0.1991, 0.1991])\n",
    "total_data = [train_dataset]\n",
    "#augmented data\n",
    "for i in range(2):\n",
    "  augmented_data = BinarySegmentationDataset(image_paths = '../input/IDRID/A.%20Segmentation/A. Segmentation/1. Original Images/a. Training Set',\n",
    "                              mask_paths = train_mask_file,\n",
    "                              mask_class=class_path,\n",
    "                              transform=aug_transform,\n",
    "                              masktransform=mask_aug_transform,\n",
    "                              mean=[0.2816, 0.2817, 0.2816],\n",
    "                              std=[0.1992, 0.1991, 0.1991])\n",
    "  total_data.append(augmented_data)\n",
    "\n",
    "total_train_dataset = ConcatDataset(total_data)\n",
    "\n",
    "test_dataset = BinarySegmentationDataset(image_paths = '../input/IDRID/A.%20Segmentation/A. Segmentation/1. Original Images/b. Testing Set',\n",
    "                              mask_paths = test_mask_file,\n",
    "                              mask_class=class_path,\n",
    "                              transform=transform,\n",
    "                              masktransform=mask_transform,\n",
    "                              mean=[0.2816, 0.2817, 0.2816],\n",
    "                              std=[0.1992, 0.1991, 0.1991])\n",
    "\n",
    "# Initialize the dataloader\n",
    "train_dataloader = DataLoader(total_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_dataset(train_dataloader, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_frequencies, pos_weight = calculate_class_weights(2, train_dataloader)\n",
    "print(class_frequencies, pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "criterion_list = [torch.nn.BCEWithLogitsLoss(),\n",
    "                  torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight).to(device)),\n",
    "                  torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor(4, dtype=torch.float32).to(device)),\n",
    "                  DiceLoss(),\n",
    "                  FocalLoss(weights=torch.tensor(4).to(device)),\n",
    "                  TverskyLoss(),\n",
    "                  TverskyLoss(0.25),\n",
    "                  TverskyLoss(0.75),\n",
    "                  FocalTverskyLoss(),\n",
    "                  FocalTverskyLoss(0.25),\n",
    "                  FocalTverskyLoss(0.75),\n",
    "                  LogCoshDice()]\n",
    "\n",
    "model_names = [\"BCEnoPOS\",\n",
    "               \"BCECalcPOS\",\n",
    "               \"BCE4POS\",\n",
    "               \"DiceLoss\",\n",
    "               \"FocalLoss\",\n",
    "               \"TverskyLossBase\",\n",
    "               \"TverskyLoss25\",\n",
    "               \"TverskyLoss75\",\n",
    "               \"FocalTverskyLossBase\",\n",
    "               \"FocalTverskyLoss25\",\n",
    "               \"FocalTverskyLoss75\",\n",
    "               \"LogCoshDice\"]\n",
    "\n",
    "loss_dataframe = pd.DataFrame()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "for idx, criterion in enumerate(criterion_list):\n",
    "    #initialise new Unet model and optimiser\n",
    "    model = uNetModel(1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    print(f\"Training {model_names[idx]} model\")\n",
    "    #train unet for the specific criterion\n",
    "    training_loss = train_unet(model, \n",
    "                               criterion, \n",
    "                               optimizer,\n",
    "                               train_dataloader,\n",
    "                               50,\n",
    "                               device,\n",
    "                               \"lesion/binary\",\n",
    "                               model_names[idx])\n",
    "    \n",
    "    loss_dataframe[model_names[idx]] = training_loss\n",
    "\n",
    "    #clear cuda memory\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "#saving data\n",
    "loss_dataframe.to_csv('results/lesion_binary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_names = [\"binary/BCEnoPOS\",\n",
    "               \"binary/BCECalcPOS\",\n",
    "               \"binary/BCE4POS\",\n",
    "               \"binary/DiceLoss\",\n",
    "               \"binary/FocalLoss\",\n",
    "               \"binary/TverskyLossBase\",\n",
    "               \"binary/TverskyLoss25\",\n",
    "               \"binary/TverskyLoss75\",\n",
    "               \"binary/FocalTverskyLossBase\",\n",
    "               \"binary/FocalTverskyLoss25\",\n",
    "               \"binary/FocalTverskyLoss75\",\n",
    "               \"binary/LogCoshDice\"]\n",
    "\n",
    "models_list = []\n",
    "\n",
    "#loading models\n",
    "for model_name in model_names:\n",
    "    model = torch.load(f\"../models/lesion/{model_name}.pt\")\n",
    "    model_last = torch.load(f\"../models/lesion/{model_name}_50.pt\")\n",
    "    models_list.append([model, model_last])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['model', 'dice coefficient', 'jaccard index'])\n",
    "\n",
    "for idx, (best_model, last_model) in enumerate(models_list):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    #get the dice coefficient and jaccard index for best and last model\n",
    "    b_dice_coe, b_jaccard = eval_unet(best_model, False, test_dataloader, device)\n",
    "    l_dice_coe, l_jaccard = eval_unet(last_model, False, test_dataloader, device)\n",
    "\n",
    "    #store results\n",
    "    results.loc[len(results)] = [f\"{model_names[idx]}_best\", b_dice_coe, b_jaccard]\n",
    "    results.loc[len(results)] = [f\"{model_names[idx]}_last\", l_dice_coe, l_jaccard]\n",
    "\n",
    "#saving data\n",
    "results.to_csv('results/lesion_binary_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_names = [\"binary/BCEnoPOS\",\n",
    "               \"binary/BCECalcPOS\",\n",
    "               \"binary/BCE4POS\",\n",
    "               \"binary/DiceLoss\",\n",
    "               \"binary/FocalLoss\",\n",
    "               \"binary/TverskyLossBase\",\n",
    "               \"binary/TverskyLoss25\",\n",
    "               \"binary/TverskyLoss75\",\n",
    "               \"binary/FocalTverskyLossBase\",\n",
    "               \"binary/FocalTverskyLoss25\",\n",
    "               \"binary/FocalTverskyLoss75\",\n",
    "               \"binary/LogCoshDice\"]\n",
    "\n",
    "models_list = []\n",
    "\n",
    "#loading models\n",
    "for model_name in model_names:\n",
    "    model = torch.load(f\"../models/lesion/{model_name}.pt\")\n",
    "    model_last = torch.load(f\"../models/lesion/{model_name}_50.pt\")\n",
    "    models_list.append([model, model_last])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, model_type = models_list[1]\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "#model inference\n",
    "inference_unet(best_model, False, test_dataloader, 1, device)\n",
    "inference_unet(last_model, False, test_dataloader, 1, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiClass Image Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassSegmentationDataset(Dataset):\n",
    "    def __init__(self, image_paths:str, mask_paths:str, mask_class:list, transform, masktransform, mean:list, std:list):\n",
    "        \"\"\"\n",
    "        Function to convert image folder to BINARY dataset\n",
    "\n",
    "        Args:\n",
    "            image_paths (str): string path to image folder\n",
    "            mask_paths (str): string path to mask folder\n",
    "            mask_class (list): list containing all the mask classes in the format of [[class 1 folder name, class 1 image annotation], [], ...]\n",
    "            transform: pyTorch Image transforms\n",
    "            masktransform: pyTorch Mask transforms\n",
    "            mean (list): mean to normalise image\n",
    "            std (list): std to normalise image\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.mask_class = mask_class        \n",
    "        self.transform = transform\n",
    "        self.masktransform = masktransform\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        \n",
    "        self.image_list = os.listdir(image_paths) #all image names\n",
    "        self.mask_list = []\n",
    "        for dir in mask_class:\n",
    "            self.mask_list.append(os.listdir(f\"{mask_paths}/{dir[0]}\")) #all mask names for each class \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_list[idx]\n",
    "        image_path = f\"{self.image_paths}/{image_name}\"\n",
    "        image = Image.open(image_path)  #read image\n",
    "        enhanced_image = enhance_contrast(image, False) #enhance contrast of image\n",
    "        merged_mask = np.zeros((image.size[1],image.size[0]), dtype=np.uint8) #create base mask\n",
    "\n",
    "        for class_idx, mask_dir in enumerate(self.mask_class):\n",
    "            mask_name = f\"{image_name[:-4]}_{mask_dir[1]}.tif\" #create mask name which is image_name_{class annotation}.tif\n",
    "            if mask_name in self.mask_list[class_idx]: #if inside the folder (exist)\n",
    "                mask = Image.open(f\"{self.mask_paths}/{mask_dir[0]}/{mask_name}\").convert('L') #read and convert to greyscale\n",
    "                mask_array = np.array(mask)\n",
    "                merged_mask[mask_array > 0] = class_idx+1 #obtain mask and convert them to class idx +1\n",
    "\n",
    "            else:\n",
    "                raise Exception(\"Mask image not found\")\n",
    "            \n",
    "        mask = Image.fromarray(merged_mask) #create mask image\n",
    "\n",
    "        seed = np.random.randint(2147483647) # make a seed with numpy generator\n",
    "\n",
    "        if self.transform is not None:\n",
    "            random.seed(seed) # apply this seed to img transforms\n",
    "            torch.manual_seed(seed)\n",
    "            enhanced_image = self.transform(enhanced_image)\n",
    "            random.seed(seed) # apply this seed to img transforms\n",
    "            torch.manual_seed(seed)\n",
    "            mask = self.masktransform(mask)\n",
    "\n",
    "\n",
    "        #normalising image\n",
    "        mean = torch.tensor(self.mean).reshape(-1, 1, 1)\n",
    "        std = torch.tensor(self.std).reshape(-1, 1, 1)\n",
    "\n",
    "        enhanced_image = ( enhanced_image - mean )/std\n",
    "\n",
    "        return enhanced_image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameters\n",
    "image_size = (512,512)\n",
    "batch_size = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    ToTensorWithoutScaling()\n",
    "])\n",
    "\n",
    "aug_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomVerticalFlip(0.5),\n",
    "    transforms.RandomRotation((-45,45)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "mask_aug_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomVerticalFlip(0.5),\n",
    "    transforms.RandomRotation((-45,45)),\n",
    "    ToTensorWithoutScaling()\n",
    "])\n",
    "\n",
    "train_mask_file = \"../input/IDRID/A.%20Segmentation/A. Segmentation/2. All Segmentation Groundtruths/a. Training Set\"\n",
    "test_mask_file = \"../input/IDRID/A.%20Segmentation/A. Segmentation/2. All Segmentation Groundtruths/b. Testing Set\"\n",
    "class_path = [[\"1. Microaneurysms\",\"MA\"], [\"2. Haemorrhages\",\"HE\"], [\"3. Hard Exudates\",\"EX\"], [\"4. Soft Exudates\",\"SE\"], [\"5. Optic Disc\", \"OD\"]]\n",
    "\n",
    "# Initialize your dataset - 1/3 unagumented and 2/3 augmented\n",
    "multiclass_train_dataset = MultiClassSegmentationDataset(image_paths = '../input/IDRID/A.%20Segmentation/A. Segmentation/1. Original Images/a. Training Set',\n",
    "                              mask_paths = train_mask_file,\n",
    "                              mask_class=class_path,\n",
    "                              transform=transform,\n",
    "                              masktransform=mask_transform,\n",
    "                              mean=[0.2816, 0.2817, 0.2816],\n",
    "                              std=[0.1992, 0.1991, 0.1991])\n",
    "total_data = [multiclass_train_dataset]\n",
    "#augmented data\n",
    "for i in range(2):\n",
    "  augmented_data = MultiClassSegmentationDataset(image_paths = '../input/IDRID/A.%20Segmentation/A. Segmentation/1. Original Images/a. Training Set',\n",
    "                              mask_paths = train_mask_file,\n",
    "                              mask_class=class_path,\n",
    "                              transform=aug_transform,\n",
    "                              masktransform=mask_aug_transform,\n",
    "                              mean=[0.2816, 0.2817, 0.2816],\n",
    "                              std=[0.1992, 0.1991, 0.1991])\n",
    "  total_data.append(augmented_data)\n",
    "\n",
    "multiclass_total_train_dataset = ConcatDataset(total_data)\n",
    "\n",
    "multiclass_test_dataset = MultiClassSegmentationDataset(image_paths = '../input/IDRID/A.%20Segmentation/A. Segmentation/1. Original Images/b. Testing Set',\n",
    "                              mask_paths = test_mask_file,\n",
    "                              mask_class=class_path,\n",
    "                              transform=transform,\n",
    "                              masktransform=mask_transform,\n",
    "                              mean=[0.2816, 0.2817, 0.2816],\n",
    "                              std=[0.1992, 0.1991, 0.1991])\n",
    "\n",
    "# Initialize the dataloader\n",
    "multiclass_train_dataloader = DataLoader(multiclass_total_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "multiclass_test_dataloader = DataLoader(multiclass_test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_dataset(multiclass_train_dataloader, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_frequencies, class_weights = calculate_class_weights(6, train_dataloader)\n",
    "print(class_frequencies, pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "criterion_list = [torch.nn.CrossEntropyLoss(),\n",
    "                  torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(device)),\n",
    "                  torch.nn.CrossEntropyLoss(weight=torch.tensor([1, 3, 3, 3, 3, 2], dtype=torch.float32).to(device)),\n",
    "                  DiceLoss(),\n",
    "                  FocalLoss(weights=torch.tensor([1, 3, 3, 3, 3, 2], dtype=torch.float32).to(device)),\n",
    "                  TverskyLoss(),\n",
    "                  TverskyLoss(0.25),\n",
    "                  TverskyLoss(0.75),\n",
    "                  FocalTverskyLoss(),\n",
    "                  FocalTverskyLoss(0.25),\n",
    "                  FocalTverskyLoss(0.75),\n",
    "                  LogCoshDice()]\n",
    "\n",
    "model_names = [\"CEL\",\n",
    "               \"CELCalcWeights\",\n",
    "               \"CELCustWeights\",\n",
    "               \"DiceLoss\",\n",
    "               \"FocalLoss\",\n",
    "               \"TverskyLossBase\",\n",
    "               \"TverskyLoss25\",\n",
    "               \"TverskyLoss75\",\n",
    "               \"FocalTverskyLossBase\",\n",
    "               \"FocalTverskyLoss25\",\n",
    "               \"FocalTverskyLoss75\",  \n",
    "               \"LogCoshDice\"]\n",
    "\n",
    "multiclass_loss_dataframe = pd.DataFrame()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "for idx, criterion in enumerate(criterion_list):\n",
    "    #initialise new Unet model and optimiser\n",
    "    model = uNetModel(6)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    print(f\"Training {model_names[idx]} model\")\n",
    "    #train unet for the specific criterion\n",
    "    training_loss = train_unet(model, \n",
    "                               criterion, \n",
    "                               optimizer,\n",
    "                               multiclass_train_dataloader,\n",
    "                               50,\n",
    "                               device,\n",
    "                               \"lesion/multiclass\",\n",
    "                               model_names[idx])\n",
    "    \n",
    "    multiclass_loss_dataframe[model_names[idx]] = training_loss\n",
    "    \n",
    "    #clear cuda memory\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "#saving data\n",
    "multiclass_loss_dataframe.to_csv('results/lesion_multiclass.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_names = [\"multiclass/CEL\",\n",
    "               \"multiclass/CELCalcWeights\",\n",
    "               \"multiclass/CELCustWeights\",\n",
    "               \"multiclass/DiceLoss\",\n",
    "               \"multiclass/FocalLoss\",\n",
    "               \"multiclass/TverskyLossBase\",\n",
    "               \"multiclass/TverskyLoss25\",\n",
    "               \"multiclass/TverskyLoss75\",\n",
    "               \"multiclass/FocalTverskyLossBase\",\n",
    "               \"multiclass/FocalTverskyLoss25\",\n",
    "               \"multiclass/FocalTverskyLoss75\",  \n",
    "               \"multiclass/LogCoshDice\"]\n",
    "\n",
    "models_list = []\n",
    "\n",
    "#loading models\n",
    "for model_name in model_names:\n",
    "    model = torch.load(f\"../models/lesion/{model_name}.pt\")\n",
    "    model_last = torch.load(f\"../models/lesion/{model_name}_50.pt\")\n",
    "    models_list.append([model, model_last])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['model', 'dice coefficient', 'jaccard index'])\n",
    "\n",
    "for idx, (best_model, last_model) in enumerate(models_list):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    #get the dice coefficient and jaccard index for best and last model\n",
    "    b_dice_coe, b_jaccard = eval_unet(best_model, True, multiclass_test_dataloader, device)\n",
    "    l_dice_coe, l_jaccard = eval_unet(last_model, True, multiclass_test_dataloader, device)\n",
    "\n",
    "    #store results\n",
    "    results.loc[len(results)] = [f\"{model_names[idx]}_best\", b_dice_coe, b_jaccard]\n",
    "    results.loc[len(results)] = [f\"{model_names[idx]}_last\", l_dice_coe, l_jaccard]\n",
    "\n",
    "#saving data\n",
    "results.to_csv('results/lesion_multiclass_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_names = [\"multiclass/CEL\",\n",
    "               \"multiclass/CELCalcWeights\",\n",
    "               \"multiclass/CELCustWeights\",\n",
    "               \"multiclass/DiceLoss\",\n",
    "               \"multiclass/FocalLoss\",\n",
    "               \"multiclass/TverskyLossBase\",\n",
    "               \"multiclass/TverskyLoss25\",\n",
    "               \"multiclass/TverskyLoss75\",\n",
    "               \"multiclass/FocalTverskyLossBase\",\n",
    "               \"multiclass/FocalTverskyLoss25\",\n",
    "               \"multiclass/FocalTverskyLoss75\",  \n",
    "               \"multiclass/LogCoshDice\"]\n",
    "\n",
    "models_list = []\n",
    "\n",
    "#loading models\n",
    "for model_name in model_names:\n",
    "    model = torch.load(f\"../models/lesion/{model_name}.pt\")\n",
    "    model_last = torch.load(f\"../models/lesion/{model_name}_50.pt\")\n",
    "    models_list.append([model, model_last])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, last_model = models_list[1]\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "#model inference\n",
    "inference_unet(best_model, True, multiclass_test_dataloader, 1, device)\n",
    "inference_unet(last_model, True, multiclass_test_dataloader, 1, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
