{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Creation\n",
    "\n",
    "This is the notebook to generate augmented dataset\n",
    "\n",
    "This notebook contains the following\n",
    "1. Image Pre-processing with Ben Graham Preprocessing\n",
    "2. Image Segmentation using CLAHE and openCV\n",
    "3. Image Segmentation using UNET  \n",
    "\n",
    "This notebook assumes the following project structure:\n",
    "```bash\n",
    "Root\n",
    "├── notebooks\n",
    "│   └── notebook1.ipynb\n",
    "└── input\n",
    "    └── Classification\n",
    "        ├── DDR\n",
    "        │   ├── train\n",
    "        │   ├── val\n",
    "        │   └── test\n",
    "        ├── BEN\n",
    "        │   ├── train\n",
    "        │   ├── val\n",
    "        │   └── test\n",
    "        ├── CLAHE\n",
    "        │   ├── train\n",
    "        │   ├── val\n",
    "        │   └── test\n",
    "        ├── UNET_Binary\n",
    "        │   ├── train\n",
    "        │   ├── val\n",
    "        │   └── test\n",
    "        └── UNET_Multiclass\n",
    "            ├── train\n",
    "            ├── val\n",
    "            └── test\n",
    "```\n",
    "\n",
    "If you do not have the dataset, please download it from our Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BASE Class for dataset creation\n",
    "from abc import ABC\n",
    "\n",
    "class Dataset_Creator(ABC):\n",
    "    def __init__():\n",
    "        pass\n",
    "\n",
    "    def create_dataset(self, source_path:str, dest_path:str, limit:int=None, show_output:bool=False):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Dataset into Train, Val, Test\n",
    "\n",
    "The existing dataset does not have any train_val_test_split\n",
    "\n",
    "We decided to randomly split the train, validation, test with 0.7, 0.15, 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def create_split_dataset(csv_filepath:str, image_dir:str, output_dir:str, override_del:bool=True):\n",
    "    \"\"\"\n",
    "    Function to split DDR dataset into train, val, test with a ratio of 0.7, 0.15, 0.15\n",
    "    The images will be moved to the directory of train/{label} \n",
    "    Dataset can be retrieved here: https://www.kaggle.com/datasets/mariaherrerot/ddrdataset\n",
    "\n",
    "    csv and image folder will be deleted upon completition\n",
    "\n",
    "    Args:\n",
    "        csv_filepath (str): filepath to csv file with id_code for img name and diagnosis as classification label\n",
    "        image_dir (str): folder for all the images\n",
    "        output_dir (str): folder for the train test split\n",
    "        override_del (bool, optional): Determines if should override the delete even if folder not empty. Default True.\n",
    "    \"\"\"\n",
    "    data =  pd.read_csv(csv_filepath, header=0)\n",
    "    image_names = data['id_code'].to_list()  #images name\n",
    "    label_list = data['diagnosis'].to_list()  #classification labels\n",
    "\n",
    "    #train val test split\n",
    "    train_val_images, test_images, train_val_labels, test_labels = train_test_split(image_names, label_list, test_size=0.15, random_state=42)\n",
    "    train_images, val_images, train_labels, val_labels = train_test_split(train_val_images, train_val_labels, test_size=0.15, random_state=42)\n",
    "\n",
    "    #moving to folder\n",
    "    for idx, image_paths in enumerate(train_images):\n",
    "        label_directory = os.path.join(f\"{output_dir}/train\", str(train_labels[idx]))\n",
    "        if not os.path.exists(label_directory):\n",
    "            os.makedirs(label_directory)\n",
    "        \n",
    "        shutil.move(f\"{image_dir}/{image_paths}\", f\"{label_directory}/{image_paths}\")\n",
    "    print(\"All train data moved\")\n",
    "\n",
    "    for idx, image_paths in enumerate(val_images):\n",
    "        label_directory = os.path.join(f\"{output_dir}/val\", str(val_labels[idx]))\n",
    "        if not os.path.exists(label_directory):\n",
    "            os.makedirs(label_directory)\n",
    "        \n",
    "        shutil.move(f\"{image_dir}/{image_paths}\", f\"{label_directory}/{image_paths}\")\n",
    "    print(\"All val data moved\")\n",
    "\n",
    "    for idx, image_paths in enumerate(test_images):\n",
    "        label_directory = os.path.join(f\"{output_dir}/test\", str(test_labels[idx]))\n",
    "        if not os.path.exists(label_directory):\n",
    "            os.makedirs(label_directory)\n",
    "        \n",
    "        shutil.move(f\"{image_dir}/{image_paths}\", f\"{label_directory}/{image_paths}\")\n",
    "    print(\"All test data moved\")\n",
    "\n",
    "    if override_del:\n",
    "        shutil.rmtree(image_dir)\n",
    "        os.remove(csv_filepath)\n",
    "\n",
    "    else:\n",
    "        os.rmdir(image_dir) #if image directory not empty, throws an error\n",
    "        os.remove(csv_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_split_dataset('../input/Classification/DDR/DR_grading.csv', '../input/Classification/DDR/DR_grading/DR_grading', '../input/Classification/DDR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Pre-processing with Ben Graham Preprocessing algorithm\n",
    "\n",
    "Inspired by Ben Graham who won the EyePacs Diabetic retinopathy challenge\n",
    "\n",
    "Due to computational limitations of applying the Ben Graham preprocessing as a torchvision Transforms, these were applied onto the original image and saved such that they can be used directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class Ben_process(Dataset_Creator):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def create_dataset(self, source_path:str, dest_path:str, limit:int=None, show_output:bool=False):\n",
    "        \"\"\"\n",
    "        Main function that will run image masking and processing with OpenCV and Ben Graham's Processing\n",
    "\n",
    "        Ben Graham's Processing takes overlaying a gaussian blur of an image on top of the original image.\n",
    "        This highlights the edges and increase the contrast, making it easier to determine the blood vessels and lesions.\n",
    "\n",
    "        A mask is created to remove the background of the image such that the focus is the eye.\n",
    "\n",
    "        All images in the folder will be preprocessed and saved to the correct location in it's original image size with the same image name\n",
    "\n",
    "        Args:\n",
    "            source_path (str): Source path of the folder\n",
    "            dest_path (str): Dest path of the folder\n",
    "            limit (int, optional): Maximum number of images to process. Defaults to None.\n",
    "            show_output (bool, optional): Show the processed image. Defaults to False\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #create destination directory\n",
    "        if not os.path.isdir(dest_path):\n",
    "          os.makedirs(dest_path)\n",
    "\n",
    "        #variable for scaling factor for image size  \n",
    "        scale = 500\n",
    "        folder_files = os.listdir(source_path)\n",
    "\n",
    "        for i, file in enumerate(folder_files, start=1):\n",
    "            base_image = cv2.imread(f'{source_path}/{file}') #open image\n",
    "\n",
    "            #resize image in to improve the ben graham effectiveness (specific for gaussian blur scale)\n",
    "            resize_image = cv2.resize(base_image, (224, 224))\n",
    "\n",
    "            #create background mask\n",
    "            mask = self._mask_image(resize_image)\n",
    "\n",
    "            #Ben Graham Processing\n",
    "            processed_img = cv2.addWeighted(resize_image, 4, cv2.GaussianBlur(resize_image, (0, 0), scale/30), -4, 128) #overlaps gaussian blur\n",
    "            np_zero_mask = np.zeros(processed_img.shape) #creates black background\n",
    "            cv2.circle(np_zero_mask, (processed_img.shape[1] // 2, processed_img.shape[0] // 2), int(scale * 0.9), (1, 1, 1), -1, 8, 0) #estimates a circle where the eye will be\n",
    "            enhanced_image = processed_img * np_zero_mask + 128 * (1 - np_zero_mask) #overlay a dark blue outline around the eye (used in scenarios where the mask fails)\n",
    "\n",
    "            #remove background\n",
    "            enhanced_image[mask == 255] = 0 \n",
    "            \n",
    "            if show_output:\n",
    "                result = np.hstack((resize_image, enhanced_image))\n",
    "                cv2.imshow(\"enhanced image\", result)\n",
    "                cv2.waitKey(10000)\n",
    "                cv2.destroyAllWindows()\n",
    "\n",
    "            cv2.imwrite(f\"{dest_path}/{file}\", enhanced_image) \n",
    "            print(f\"Image {i}/{len(folder_files)} processed. Output path:{dest_path}/{file}\")\n",
    "\n",
    "            if limit == None:\n",
    "                pass\n",
    "\n",
    "            elif i >= limit:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "    def _mask_image(self,image:np.ndarray):\n",
    "        \"\"\"\n",
    "        Function to created a mask of the eyeball in black against the white background\n",
    "        \n",
    "        Background of image will be white to act as the mask\n",
    "\n",
    "        Args:\n",
    "            image (np.ndarray): Image to create mask of\n",
    "\n",
    "        Return:\n",
    "            mask (np.ndarray): Maks of image\n",
    "        \"\"\"\n",
    "\n",
    "        #Convert image to grayscale to improve background removal\n",
    "        tmp = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        #Create border threshold of 5-255\n",
    "        _, mask = cv2.threshold(tmp, 5, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        #Blur the mask to smoothen threshold edges\n",
    "        mask = cv2.GaussianBlur(mask, (11,11), 0)\n",
    "\n",
    "        #Create base white background\n",
    "        white_bg = np.full_like(image, 255)\n",
    "\n",
    "        #Create mask and invert it such that we get the eyeball is black and background is white\n",
    "        mask = cv2.bitwise_not(mask)\n",
    "\n",
    "        #Overlay the mask on white background. Background will be white, eyeball will be black\n",
    "        masked_image = cv2.bitwise_or(white_bg, white_bg, mask=mask)\n",
    "\n",
    "        return masked_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = \"../input/Classification/DDR\"\n",
    "dest_path = \"../input/Classification/BEN\"\n",
    "folder_list = [\"train\", \"val\", \"test\"]\n",
    "class_lists = ['0', '1', '2', '3', '4']\n",
    "\n",
    "Ben_Processor = Ben_process()\n",
    "\n",
    "for folder_name in folder_list:\n",
    "    for class_idx in class_lists:\n",
    "        image_src_path = f\"{source_path}/{folder_name}/{class_idx}\"\n",
    "        mask_dest_path = f\"{dest_path}/{folder_name}/{class_idx}\"\n",
    "        Ben_Processor.create_dataset(image_src_path, mask_dest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Segmentation with OpenCV and Clahe\n",
    "\n",
    "Inspired by Detecting Diabetic Retinopathy in Fundus Images using Combined Enhanced Green and Value Planes (CEGVP) with k-NN:\n",
    "<br>\n",
    "https://thesai.org/Publications/ViewPaper?Volume=13&Issue=1&Code=IJACSA&SerialNo=32\n",
    "\n",
    "\n",
    "The image will have it's contrast enhanced before extracting the green channel of the image. The green channel will then undergo edge detection with different thresholds to segment the blood vessels and the lesions.\n",
    "\n",
    "The background will be black, with the extracted blood vessels in white and the lesions will be in gray.\n",
    "\n",
    "\n",
    "Due to computational limitations of applying segmentation as a torchvision Transforms, these were applied onto the original image and saved such that they can be used directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class OPENCV_segmentation(Dataset_Creator):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "   \n",
    "    def create_dataset(self, source_path:str, dest_path:str, limit:int=None, show_output:bool=False):\n",
    "        \"\"\"\n",
    "        Main function that will run image segmentation with OpenCV and CLAHE\n",
    "        Image segmentation will be run using edge detection, background will be black while the vessels are white and lesions are gray\n",
    "\n",
    "        All images in the folder will be preprocessed and saved to the correct location in it's original image size with the same image name\n",
    "\n",
    "        Args:\n",
    "            source_path (str): Source path of the folder\n",
    "            dest_path (str): Dest path of the folder\n",
    "            limit (int, optional): Maximum number of images to process. Defaults to None.\n",
    "            show_output (bool, optional): Show the processed image. Defaults to False\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        #create destination directory\n",
    "        if not os.path.isdir(dest_path):\n",
    "          os.makedirs(dest_path)\n",
    "\n",
    "        folder_files = os.listdir(source_path)\n",
    "\n",
    "        for i, file in enumerate(folder_files, start=1):\n",
    "            base_image = cv2.imread(f'{source_path}/{file}') #open image\n",
    "\n",
    "            #enhance contrast of image\n",
    "            enhanced_image = self._enhance_contrast(base_image, False)\n",
    "\n",
    "            #enhance green channel of the image\n",
    "            max_pixel_diff, enhanced_green, g = self._enhance_green_channel(image=enhanced_image,\n",
    "                                                show_channels=False,\n",
    "                                                show_enhanced=False)\n",
    "\n",
    "            #extract blood vessel\n",
    "            blood_vessel_mask = self._extract_blood_vessels(image=enhanced_green,\n",
    "                                                    show_extracted=False)\n",
    "\n",
    "            #extract lesion \n",
    "            lesion_mask = self._extract_lesion(image=g, max_pixel_diff=max_pixel_diff, show_extracted=False)\n",
    "\n",
    "            #calculate the overlap between blood_vessel and lesion mask, prioritise blood vessel\n",
    "            overlap_mask = cv2.bitwise_and(blood_vessel_mask, lesion_mask)\n",
    "            lesion_mask = np.subtract(lesion_mask, overlap_mask) #remove overlapping parts from lesion mask\n",
    "            lesion_mask[lesion_mask == 255] = 100 #convert lesion mask to gray\n",
    "\n",
    "            merged_mask = cv2.bitwise_or(blood_vessel_mask, lesion_mask)\n",
    "            \n",
    "            rgb_mask = cv2.cvtColor(merged_mask,cv2.COLOR_GRAY2RGB) \n",
    "\n",
    "            if show_output:\n",
    "                resized = cv2.resize(rgb_mask, (224,224))\n",
    "                result = np.hstack((cv2.resize(base_image, (224,224)), resized))\n",
    "                cv2.imshow(\"output\", result)\n",
    "                cv2.waitKey(10000)\n",
    "                cv2.destroyAllWindows()\n",
    "\n",
    "            cv2.imwrite(f\"{dest_path}/{file}\", rgb_mask) \n",
    "            print(f\"Image {i}/{len(folder_files)} processed. Output path:{dest_path}/{file}\")\n",
    "\n",
    "            if limit == None:\n",
    "                pass\n",
    "\n",
    "            elif i >= limit:\n",
    "                break\n",
    "\n",
    "\n",
    "    def _enhance_contrast(self, image:np.ndarray, show_enchanced:bool=False):\n",
    "        \"\"\"\n",
    "        Function to enhance the contrast of image using CLAHE\n",
    "\n",
    "        Args:\n",
    "            image (np.ndarray): Image to be processed\n",
    "            show_enchanced (bool): Boolean to show before and after image. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            enhanced image (np.ndarray)\n",
    "        \"\"\"\n",
    "        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB) #convert rgb image to lab channels\n",
    "        L, a, b = cv2.split(lab) #split lab channels\n",
    "\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8)) #create Contrast Limited Adaptive Histogram Equalization (CLAHE)\n",
    "        cL = clahe.apply(L) #apply to lab channel to increase contrast\n",
    "\n",
    "        new_img = cv2.merge((cL,a,b)) #create new lab enhanced image\n",
    "        enhanced_img = cv2.cvtColor(new_img, cv2.COLOR_LAB2BGR) #convert from lab to rgb\n",
    "\n",
    "        if show_enchanced:\n",
    "          result = np.hstack((cv2.resize(image, (224,224)), (cv2.resize(enhanced_img, (224,224)))))\n",
    "          cv2.imshow(\"enhanced image\", result)\n",
    "          cv2.waitKey(10000)\n",
    "          cv2.destroyAllWindows()\n",
    "\n",
    "        return enhanced_img\n",
    "\n",
    "\n",
    "    def _enhance_green_channel(self, image:np.ndarray, kernel_size:tuple=(75,75), show_channels:bool=False, show_enhanced:bool=False):\n",
    "        \"\"\"\n",
    "        Function to enhance the constrast of the GREEN channel of the image by doing noise extraction\n",
    "\n",
    "        Args:\n",
    "            image (np.ndarray): Image to be processed\n",
    "            kernel_size (tuple, optional): Kernel size for . Defaults to (75,75).\n",
    "            show_channels (bool, optional): Boolean to show the r,g and b channel of image. Defaults to False.\n",
    "            show_enhanced (bool, optional): Boolean to show enhanced image. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            max pixel diff (int): maximum pixel difference in enhanced image\n",
    "            enhanced green: enhanced green channel image\n",
    "            g: green channel image without enhancement\n",
    "        \"\"\"\n",
    "        b,g,r = cv2.split(image)\n",
    "        if show_channels:\n",
    "          cv2.imshow(\"blue channel\", b)\n",
    "          cv2.imshow(\"green channel\", g)\n",
    "          cv2.imshow(\"red channel\", r)\n",
    "\n",
    "        kernel = np.ones(kernel_size, np.uint8) #creating kernel\n",
    "        opening = cv2.morphologyEx(g, cv2.MORPH_OPEN, kernel) #morphological transformations to remove noise, smoothens the image\n",
    "        morph_green = cv2.subtract(g, opening) #subtract to extract out the outliers/noise that were removed, highlights the key areas and increases contrast\n",
    "    \n",
    "        max_pixel_diff = np.max(morph_green)\n",
    "\n",
    "        min_intensity = np.min(morph_green)\n",
    "        max_intensity = np.max(morph_green)\n",
    "\n",
    "        enhanced_green = ((morph_green - min_intensity) / (max_intensity - min_intensity)) * 255 #normalise image\n",
    "        enhanced_green = enhanced_green.astype(np.uint8)\n",
    "\n",
    "        if show_enhanced:\n",
    "          result = np.hstack((cv2.resize(g, (224,224)), cv2.resize(enhanced_green, (224,224))))\n",
    "          cv2.imshow(\"enhanced green channel\", result)\n",
    "          cv2.waitKey(10000)\n",
    "          cv2.destroyAllWindows()\n",
    "\n",
    "        return max_pixel_diff, enhanced_green, g\n",
    "\n",
    "\n",
    "    def _extract_blood_vessels(self, image:np.ndarray, block_size:int=31, calculated_mean:int=21, area_limit:int=3, show_extracted:bool=False):\n",
    "        \"\"\"\n",
    "        Function to extract out blood vessels through the use of edge detection\n",
    "\n",
    "        Args:\n",
    "            image (np.ndarray): Image to be processed\n",
    "            block_size (int, optional): Size of local region around each pixel used for adaptive thresholding, the higher the more global the image. MUST be an odd number . Defaults to 31.\n",
    "            calculated_mean (int, optional): Constant value added to each pixel. Defaults to 21.\n",
    "            area_limit (int, optional): Minimum limit of the area of mask to be considered a mask. Defaults to 3.\n",
    "            show_extracted (bool, optional): Boolean to show extracted vessel mask. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            mask (np.ndaray): Extracted vessel mask\n",
    "        \"\"\"\n",
    "        if block_size % 2 ==0:\n",
    "            raise Exception(\"block_size must be an odd integer\")\n",
    "\n",
    "        while True:\n",
    "            #adaptive threshold for edge detection\n",
    "            adaptive_threshold = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, block_size, calculated_mean)\n",
    "            contours, _ = cv2.findContours(adaptive_threshold, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) #edge detection\n",
    "\n",
    "            #sort contours by max area\n",
    "            contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "            contours = [cnt for cnt in contours if cv2.contourArea(cnt) > area_limit] #sieve and extract contours larger than limit\n",
    "            if len(contours) > 0:\n",
    "              max_contour_area = max([cv2.contourArea(cnt) for cnt in contours]) #get maximum contours\n",
    "              if max_contour_area >= 0.5 * image.shape[0] * image.shape[1]: #if the max coutour is larger than 50% of the image (edges not defined enough, repeat and increase block size and mean)\n",
    "                  block_size += 2\n",
    "                  calculated_mean += 3\n",
    "              \n",
    "              else:\n",
    "                 break\n",
    "            \n",
    "            else:\n",
    "                break\n",
    "\n",
    "        #create base mask\n",
    "        mask = np.zeros_like(image)\n",
    "        cv2.drawContours(mask, contours, -1, (255), thickness=cv2.FILLED) #update mask with contours\n",
    "\n",
    "        if show_extracted:\n",
    "          result = np.hstack((cv2.resize(image, (224,224)), cv2.resize(mask, (224,224))))\n",
    "          cv2.imshow(\"vein extraction\", result)\n",
    "          cv2.waitKey(10000)\n",
    "          cv2.destroyAllWindows()\n",
    "\n",
    "        return mask\n",
    "\n",
    "\n",
    "    def _extract_lesion(self, image: np.ndarray, max_pixel_diff: int = 125, show_extracted: bool = False):\n",
    "        \"\"\"\n",
    "        Function to extract out lesions through the use of edge detection\n",
    "\n",
    "        Args:\n",
    "            image (np.ndarray): Green channel of image that isn't processed\n",
    "            max_pixel_diff (int, optional): Max pixel diff from processing. Defaults to 125.\n",
    "            show_extracted (bool, optional): Boolean to show extracted lesion mask. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            mask (np.ndarray): Extracted lesion mask\n",
    "        \"\"\"\n",
    "        # Initialize variables to None\n",
    "        binary = binary2 = binary3 = None\n",
    "\n",
    "        # Conditional logic for threshold boundaries depending on max_pixel_diff due to different lighting conditions\n",
    "        # Binary: all somewhat light parts of the image beyond a certain threshold\n",
    "        # Binary2: stricter threshold, aim is for Binary2 - Binary to just return the lesions without any light spots on image\n",
    "        # Binary3: extremely strict threshold, only return lightest part of image, aim to retrieve optic disk\n",
    "        if max_pixel_diff < 165:\n",
    "            _, binary = cv2.threshold(image, max_pixel_diff, 255, cv2.THRESH_BINARY)\n",
    "            _, binary2 = cv2.threshold(image, max_pixel_diff, 255, cv2.THRESH_BINARY)\n",
    "            _, binary3 = cv2.threshold(image, max_pixel_diff * 1.1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        elif 165<= max_pixel_diff <200:\n",
    "          _, binary = cv2.threshold(image, max_pixel_diff*0.8, 255, cv2.THRESH_BINARY)\n",
    "          _, binary2 = cv2.threshold(image, max_pixel_diff*0.85, 255, cv2.THRESH_BINARY)\n",
    "          _, binary3 = cv2.threshold(image, max_pixel_diff*1.1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        elif 200<= max_pixel_diff <215:\n",
    "          _, binary = cv2.threshold(image, max_pixel_diff*0.7, 255, cv2.THRESH_BINARY)\n",
    "          _, binary2 = cv2.threshold(image, max_pixel_diff*0.75, 255, cv2.THRESH_BINARY)\n",
    "          _, binary3 = cv2.threshold(image, max_pixel_diff*0.8, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        elif 215 <= max_pixel_diff < 230:\n",
    "          _, binary = cv2.threshold(image, max_pixel_diff*0.75, 255, cv2.THRESH_BINARY)\n",
    "          _, binary2 = cv2.threshold(image, max_pixel_diff*0.85, 255, cv2.THRESH_BINARY)\n",
    "          _, binary3 = cv2.threshold(image, max_pixel_diff*0.9, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        elif 230<= max_pixel_diff <245:\n",
    "          _, binary = cv2.threshold(image, max_pixel_diff*0.45, 255, cv2.THRESH_BINARY)\n",
    "          _, binary2 = cv2.threshold(image, max_pixel_diff*0.5, 255, cv2.THRESH_BINARY)\n",
    "          _, binary3 = cv2.threshold(image, max_pixel_diff, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        elif max_pixel_diff >245:\n",
    "          _, binary = cv2.threshold(image, max_pixel_diff*0.7, 255, cv2.THRESH_BINARY)\n",
    "          _, binary2 = cv2.threshold(image, max_pixel_diff*0.75, 255, cv2.THRESH_BINARY)\n",
    "          _, binary3 = cv2.threshold(image, max_pixel_diff, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        #retrieve contours\n",
    "        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) \n",
    "        contours2, _ = cv2.findContours(binary2, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        contours3, _ = cv2.findContours(binary3, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        #generate empty mask\n",
    "        mask = np.zeros_like(image)\n",
    "        mask2 = np.zeros_like(image)\n",
    "        mask3 = np.zeros_like(image)\n",
    "\n",
    "        #adding contours to the different mask accordingly\n",
    "        cv2.drawContours(mask, contours, -1, (255), thickness=cv2.FILLED)\n",
    "        cv2.drawContours(mask2, contours2, -1, (255), thickness=cv2.FILLED)\n",
    "        cv2.drawContours(mask3, contours3, -1, (255), thickness=cv2.FILLED) #optic disk\n",
    "\n",
    "        #subtracting binary from binary2 to extract just the small lesions\n",
    "        merge_mask = np.subtract(mask, mask2)\n",
    "        #adding the optic disk\n",
    "        merge_mask = np.add(merge_mask, mask3)\n",
    "\n",
    "        if show_extracted:\n",
    "          result = np.hstack((cv2.resize(image, (224,224)), cv2.resize(merge_mask, (224,224))))\n",
    "          cv2.imshow(\"outlier extraction\", result)\n",
    "          cv2.waitKey(10000)\n",
    "          cv2.destroyAllWindows()\n",
    "\n",
    "        return merge_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = \"../input/Classification/DDR\"\n",
    "dest_path = \"../input/Classification/OPENCV\"\n",
    "folder_list = [\"train\", \"val\", \"test\"]\n",
    "class_lists = ['0', '1', '2', '3', '4']\n",
    "\n",
    "CLAHE_segmenter = OPENCV_segmentation()\n",
    "\n",
    "for folder_name in folder_list:\n",
    "    for class_idx in class_lists:\n",
    "        image_src_path = f\"{source_path}/{folder_name}/{class_idx}\"\n",
    "        mask_dest_path = f\"{dest_path}/{folder_name}/{class_idx}\"\n",
    "        CLAHE_segmenter.create_dataset(image_src_path, mask_dest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Segmentation with UNET\n",
    "\n",
    "Requires: 2 pretrained UNET model, one for vessel segmentation and one for lesion segmentation\n",
    "\n",
    "There are 2 ways to create the dataset\n",
    "1. Binary class: vessel and lesion segmentation will return a binary mask which will be merged\n",
    "2. Multi Class: vessel segmentation will be deemed class 1 while the lesion segmentation will take class 2-6. Background will be black while the different classes will get varying shades of gray depending on their class index.\n",
    "\n",
    "Due to computational limitations of applying segmentation as a torchvision Transforms, these were applied onto the original image and saved such that they can be used directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(torch.nn.Module):\n",
    "  \"\"\"convolutional block for that UNET\"\"\"\n",
    "  def __init__(self, in_channels:int, out_channels:int):\n",
    "    super(conv_block, self).__init__()\n",
    "    self.conv1 = torch.nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "    self.bn1 = torch.nn.BatchNorm2d(out_channels)\n",
    "    self.conv2 = torch.nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "    self.bn2 = torch.nn.BatchNorm2d(out_channels)\n",
    "    self.relu = torch.nn.ReLU()\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    x = self.relu(self.bn1(self.conv1(inputs)))\n",
    "    x = self.relu(self.bn2(self.conv2(x)))\n",
    "    return x\n",
    "  \n",
    "class encoder_block(torch.nn.Module):\n",
    "  \"\"\" \n",
    "  encoder block that includes convolutional block and maxpooling\n",
    "  returns both values before maxpool and after maxpool (for skip connections)\n",
    "  \"\"\" \n",
    "  def __init__(self, in_channels:int, out_channels:int):\n",
    "    super(encoder_block, self).__init__()\n",
    "    self.conv = conv_block(in_channels, out_channels)\n",
    "    self.maxpool = torch.nn.MaxPool2d((2,2))\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    x = self.conv(inputs)\n",
    "    p = self.maxpool(x)\n",
    "    return x, p\n",
    "\n",
    "class decoder_block(torch.nn.Module):\n",
    "  \"\"\"\n",
    "  decoder block that upsamples images and takes in skip connections\n",
    "  \"\"\"\n",
    "  def __init__(self, in_channels:int, out_channels:int):\n",
    "    super(decoder_block, self).__init__()\n",
    "    self.upsample = torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "    self.conv = conv_block(out_channels+out_channels, out_channels)\n",
    "\n",
    "  def forward(self, inputs, skip_connections):\n",
    "    x = self.upsample(inputs)\n",
    "    x = torch.cat((x, skip_connections), 1)\n",
    "    return self.conv(x)\n",
    "\n",
    "class uNetModel(torch.nn.Module):\n",
    "  \"\"\"UNET architecture\"\"\"\n",
    "  def __init__(self, n_classes):\n",
    "    super(uNetModel, self).__init__()\n",
    "    #--------------------------\n",
    "    # Encoder\n",
    "    #--------------------------\n",
    "    self.encoder1 = encoder_block(3, 64)\n",
    "    self.encoder2 = encoder_block(64, 128)\n",
    "    self.encoder3 = encoder_block(128, 256)\n",
    "    self.encoder4 = encoder_block(256, 512)\n",
    "\n",
    "    #--------------------------\n",
    "    # Bottleneck\n",
    "    #--------------------------\n",
    "    self.bottleneck = conv_block(512, 1024)\n",
    "\n",
    "    #--------------------------\n",
    "    # Encoder\n",
    "    #--------------------------\n",
    "    self.decoder1 = decoder_block(1024, 512)\n",
    "    self.decoder2 = decoder_block(512, 256)\n",
    "    self.decoder3 = decoder_block(256, 128)\n",
    "    self.decoder4 = decoder_block(128, 64)\n",
    "\n",
    "    #--------------------------\n",
    "    # Classifier\n",
    "    #--------------------------\n",
    "    self.classifier = torch.nn.Conv2d(64, n_classes, 1)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    x1, p1 = self.encoder1(inputs)\n",
    "    x2, p2 = self.encoder2(p1)\n",
    "    x3, p3 = self.encoder3(p2)\n",
    "    x4, p4 = self.encoder4(p3)\n",
    "    b = self.bottleneck(p4)\n",
    "\n",
    "    d1 = self.decoder1(b, x4)\n",
    "    d2 = self.decoder2(d1, x3)\n",
    "    d3 = self.decoder3(d2, x2)\n",
    "    d4 = self.decoder4(d3, x1)\n",
    "\n",
    "    output = self.classifier(d4)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET_segmentation(Dataset_Creator):\n",
    "    def __init__(self, multiclass:bool, vessel_unet_path:str, lesion_unet_path:str, device:str, mean=[0.2816, 0.2817, 0.2816], std=[0.1992, 0.1991, 0.1991] ):\n",
    "        \"\"\"\n",
    "        Intialising unet segmentation\n",
    "\n",
    "        Args:\n",
    "            multiclass (bool): Boolean if the dataset will be binary or multiclass\n",
    "            vessel_unet_path (str): Path to UNET vessel model\n",
    "            lesion_unet_path (str): Path to UNET lesion model\n",
    "            device (str): cuda or cpu\n",
    "            mean (list, optional): mean to normalise image UNET model was trained on. Defaults to [0.2816, 0.2817, 0.2816].\n",
    "            std (list, optional): std to normalise image UNET model was trained on. Defaults to [0.1992, 0.1991, 0.1991].\n",
    "        \"\"\"\n",
    "        self.multiclass = multiclass\n",
    "        self.vessel_unet = torch.load(vessel_unet_path).to(device)\n",
    "        self.lesion_unet = torch.load(lesion_unet_path).to(device)\n",
    "        self.mean =  torch.tensor(mean).reshape(-1, 1, 1)\n",
    "        self.std = torch.tensor(std).reshape(-1, 1, 1)\n",
    "        self.device = device\n",
    "\n",
    "        self.colour_map = {\n",
    "            0: 0,  # Black for background (label 0)\n",
    "            1: 255,  # White for label 1\n",
    "            2: 128,  # Gray for label 2\n",
    "            3: 100,  # Gray for label 3\n",
    "            4: 150,  # Gray for label 4\n",
    "            5: 200,  # Gray for label 5\n",
    "            6: 50 \n",
    "        }\n",
    "        \n",
    "\n",
    "    def create_dataset(self, source_path:str, dest_path:str, limit:int=None, show_output:bool=False):\n",
    "        \"\"\"\n",
    "        Main function that will run image segmentation with UNET\n",
    "\n",
    "        All images in the folder will be preprocessed and saved to the correct location in 512x512 with the same image name\n",
    "\n",
    "        Args:\n",
    "            source_path (str): Source path of the folder\n",
    "            dest_path (str): Dest path of the folder\n",
    "            limit (int, optional): Maximum number of images to process. Defaults to None.\n",
    "            show_output (bool, optional): Show the processed image. Defaults to False\n",
    "        \"\"\"\n",
    "        #create destination directory\n",
    "        if not os.path.isdir(dest_path):\n",
    "          os.makedirs(dest_path)\n",
    "\n",
    "        folder_files = os.listdir(source_path)\n",
    "\n",
    "        for i, file in enumerate(folder_files, start=1):\n",
    "            base_image = Image.open(f'{source_path}/{file}') #open image\n",
    "            resized_image = base_image.resize((512,512))\n",
    "            enhanced_image = self._enhance_contrast(resized_image, False)\n",
    "            tensor_image = torchvision.transforms.ToTensor()(enhanced_image)\n",
    "            #normalising image\n",
    "            input = ( tensor_image - self.mean )/self.std #normalise image\n",
    "\n",
    "            #run vessel UNET and get the predictions with sigmoid\n",
    "            vessel_mask = self.vessel_unet(input.unsqueeze(0).to(self.device))\n",
    "            vessel_outputs = torch.nn.functional.sigmoid(vessel_mask)\n",
    "            vessel_predictions = (vessel_outputs > 0.5)\n",
    "            vessel_predictions = vessel_predictions.cpu().numpy()\n",
    "            \n",
    "            #run lesion UNET\n",
    "            lesion_mask = self.lesion_unet(input.unsqueeze(0).to(self.device))\n",
    "            \n",
    "            result = np.zeros((512, 512), dtype=np.uint8)\n",
    "\n",
    "            if self.multiclass:\n",
    "                #get multiclass prediction\n",
    "                lesion_outputs = torch.nn.functional.softmax(lesion_mask)\n",
    "                lesion_predictions = torch.argmax(lesion_outputs, dim=1).cpu().numpy()\n",
    "                lesion_predictions[lesion_predictions >= 1] += 1 #increase the class index such that it is now 0,2,3,4,5, to prevent overlap with vessel\n",
    "                #merge mask, prioritise the vessel segmentation over lesion\n",
    "                merged_mask = np.where(vessel_predictions != 0, vessel_predictions, lesion_predictions)\n",
    "                \n",
    "                #update the colour mapping\n",
    "                for label, intensity in self.colour_map.items():\n",
    "                    result[merged_mask.squeeze() == label] = intensity\n",
    "            \n",
    "                if show_output:\n",
    "                    merged_result = np.hstack((np.array(resized_image), result))\n",
    "                    cv2.imshow(\"mask\", merged_result)\n",
    "                    cv2.waitKey(10000)\n",
    "                    cv2.destroyAllWindows()\n",
    "                    \n",
    "                #save image\n",
    "                cv2.imwrite(f\"{dest_path}/{file}\", cv2.cvtColor(result, cv2.COLOR_RGB2BGR))\n",
    "                print(f\"Image {i}/{len(folder_files)} processed. Output path:{dest_path}/{file}\")\n",
    "\n",
    "            elif not self.multiclass:\n",
    "                #get binary prediction\n",
    "                lesion_outputs = torch.nn.functional.sigmoid(lesion_mask)\n",
    "                lesion_predictions = (lesion_outputs > 0.5).cpu().numpy()\n",
    "\n",
    "                #merge mask, prioritise the vessel segmentation over lesion\n",
    "                merged_mask = np.where(vessel_predictions != 0, vessel_predictions, lesion_predictions)\n",
    "                result[merged_mask.squeeze() == 1] = 255 #convert binary\n",
    "                \n",
    "                if show_output:\n",
    "                    merged_result = np.hstack((np.array(resized_image), result))\n",
    "                    cv2.imshow(\"mask\", merged_result)\n",
    "                    cv2.waitKey(10000)\n",
    "                    cv2.destroyAllWindows()\n",
    "\n",
    "                #save image\n",
    "                cv2.imwrite(f\"{dest_path}/{file}\", cv2.cvtColor(result, cv2.COLOR_RGB2BGR))\n",
    "                print(f\"Image {i}/{len(folder_files)} processed. Output path:{dest_path}/{file}\")\n",
    "            \n",
    "            if limit == None:\n",
    "                pass\n",
    "\n",
    "            elif i >= limit:\n",
    "                break\n",
    "\n",
    "\n",
    "    def _enhance_contrast(self, image:Image, show_image:bool):    \n",
    "        \"\"\"\n",
    "        Function to enhance the contrast of the GREEN channel of a RGB image with CLAHE\n",
    "\n",
    "        Args:\n",
    "            image (Image): Image\n",
    "            show_image (bool): Boolean to show the enhanced image \n",
    "\n",
    "        Returns:\n",
    "            enhanced_image (Image): enhanced image \n",
    "        \"\"\"\n",
    "        image = np.array(image)\n",
    "        r,g,b = cv2.split(image) #extract out green channel\n",
    "\n",
    "        green_channel = cv2.cvtColor(g, cv2.COLOR_GRAY2RGB)  # convert green channel to RGB\n",
    "        lab = cv2.cvtColor(green_channel, cv2.COLOR_RGB2LAB)  # convert rgb image to lab channels\n",
    "\n",
    "        L, a, b = cv2.split(lab)  # split lab channels\n",
    "\n",
    "        clahe = cv2.createCLAHE(clipLimit=3, tileGridSize=(8, 8))  # create CLAHE\n",
    "        cL = clahe.apply(L)  # apply CLAHE to enhance contrast\n",
    "\n",
    "        new_img = cv2.merge((cL, a, b))  # create new lab enhanced image\n",
    "        enhanced_img = cv2.cvtColor(new_img, cv2.COLOR_LAB2RGB) # convert from lab to rgb\n",
    "\n",
    "\n",
    "        if show_image:\n",
    "            resized_enhanced_img = cv2.resize(enhanced_img, (224,224))\n",
    "            cv2.imshow(\"\", resized_enhanced_img)\n",
    "            cv2.waitKey(10000)\n",
    "            cv2.destroyAllWindows()\n",
    "        \n",
    "        return Image.fromarray(enhanced_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearing GPU memory\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = \"../input/Classification/DDR\"\n",
    "dest_path = \"../input/Classification/UNET_Binary\"\n",
    "folder_list = [\"train\", \"val\", \"test\"]\n",
    "class_lists = ['0', '1', '2', '3', '4']\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "uNet_segmenter = UNET_segmentation(multiclass=False, \n",
    "                                   vessel_unet_path = \"../models/vessel/FocalTverskyLossBase.pt\",\n",
    "                                   lesion_unet_path=\"../models/lesion/BinaryDiceLoss.pt\",\n",
    "                                   device=device)\n",
    "\n",
    "for folder_name in folder_list:\n",
    "    for class_idx in class_lists:\n",
    "        image_src_path = f\"{source_path}/{folder_name}/{class_idx}\"\n",
    "        mask_dest_path = f\"{dest_path}/{folder_name}/{class_idx}\"\n",
    "        uNet_segmenter.create_dataset(image_src_path, mask_dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = \"../input/Classification/DDR\"\n",
    "dest_path = \"../input/Classification/UNET_Multiclass\"\n",
    "folder_list = [\"train\", \"val\", \"test\"]\n",
    "class_lists = ['0', '1', '2', '3', '4']\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Clearing GPU memory\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "uNet_segmenter = UNET_segmentation(multiclass=True, \n",
    "                                   vessel_unet_path = \"../models/vessel/FocalTverskyLossBase.pt\",\n",
    "                                   lesion_unet_path=\"../models/lesion/MulticlassDiceLoss.pt\",\n",
    "                                   device=device)\n",
    "\n",
    "for folder_name in folder_list:\n",
    "    for class_idx in class_lists:\n",
    "        image_src_path = f\"{source_path}/{folder_name}/{class_idx}\"\n",
    "        mask_dest_path = f\"{dest_path}/{folder_name}/{class_idx}\"\n",
    "        uNet_segmenter.create_dataset(image_src_path, mask_dest_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
