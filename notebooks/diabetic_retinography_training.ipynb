{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Creation\n",
    "\n",
    "This is the notebook to train the classification of Diabetic Retinography with CNNs\n",
    "\n",
    "This notebook contains the following\n",
    "1. Dataset Creation and Augmentation\n",
    "2. Train and Eval Functions\n",
    "3. CNN Class Models (InceptionV3, ResNet50, ResNet152, EfficientNet, DenseNet, VGG16, MaxViT)\n",
    "\n",
    "This notebook assumes the following project structure:\n",
    "```bash\n",
    "Root\n",
    "├── notebooks\n",
    "│   └── notebook1.ipynb\n",
    "└── input\n",
    "    └── Data\n",
    "        ├── DDR\n",
    "        │   ├── Train\n",
    "        │   └── Test\n",
    "        ── BEN\n",
    "        │   ├── Train\n",
    "        │   └── Test\n",
    "        ├── CLAHE\n",
    "        │   ├── Train\n",
    "        │   └── Test\n",
    "        ├── UNET_Binary\n",
    "        │   ├── Train\n",
    "        │   └── Test\n",
    "        └── UNET_Multiclass\n",
    "            ├── Train\n",
    "            └── Test\n",
    "```\n",
    "\n",
    "If you do not have the dataset, please download it from our Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary Imports\n",
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter required\n",
    "image_size = (299,299)\n",
    "batch_size = 64\n",
    "\n",
    "# Defining Train Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Flip horizontally with a 50% probability\n",
    "    transforms.RandomVerticalFlip(p=0.5),  # Flip vertically with a 50% probability\n",
    "    transforms.RandomAffine(\n",
    "        degrees=360,  # Rotation\n",
    "        translate=(0.1, 0.1),  # Translation\n",
    "        scale=(0.8, 1.2) #Zooming\n",
    "    ),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "# Defining Evaluation Transforms, no data augmentation\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ben Graham dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets for training and validation\n",
    "ben_train_dataset = datasets.ImageFolder(\n",
    "                    root='../input/Classification/BEN/train', \n",
    "                    transform=train_transform\n",
    "                    )\n",
    "ben_val_dataset = datasets.ImageFolder(\n",
    "                    root='../input/Classification/BEN/val', \n",
    "                    transform=eval_transform\n",
    "                    )\n",
    "ben_test_dataset = datasets.ImageFolder(\n",
    "                    root='../input/Classification/BEN/test', \n",
    "                    transform=eval_transform\n",
    "                    )\n",
    "\n",
    "# Create PyTorch dataloaders for training and validation\n",
    "ben_train_dataloader = torch.utils.data.DataLoader(\n",
    "                    ben_train_dataset,\n",
    "                    batch_size=batch_size, \n",
    "                    shuffle=True\n",
    "                    )\n",
    "ben_val_dataloader = torch.utils.data.DataLoader(\n",
    "                    ben_val_dataset, \n",
    "                    batch_size=batch_size, \n",
    "                    shuffle=True\n",
    "                    )\n",
    "ben_test_dataloader = torch.utils.data.DataLoader(\n",
    "                    ben_test_dataset, \n",
    "                    batch_size=batch_size, \n",
    "                    shuffle=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPENCV dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets for training and validation\n",
    "opencv_train_dataset = datasets.ImageFolder(\n",
    "                    root='../input/Classification/OPENCV/train', \n",
    "                    transform=train_transform\n",
    "                    )\n",
    "opencv_val_dataset = datasets.ImageFolder(\n",
    "                    root='../input/Classification/OPENCV/val', \n",
    "                    transform=eval_transform\n",
    "                    )\n",
    "opencv_test_dataset = datasets.ImageFolder(\n",
    "                    root='../input/Classification/OPENCV/test', \n",
    "                    transform=eval_transform\n",
    "                    )\n",
    "\n",
    "# Create PyTorch dataloaders for training and validation\n",
    "opencv_train_dataloader = torch.utils.data.DataLoader(\n",
    "                    opencv_train_dataset,\n",
    "                    batch_size=batch_size, \n",
    "                    shuffle=True\n",
    "                    )\n",
    "opencv_val_dataloader = torch.utils.data.DataLoader(\n",
    "                    opencv_val_dataset, \n",
    "                    batch_size=batch_size, \n",
    "                    shuffle=True\n",
    "                    )\n",
    "opencv_test_dataloader = torch.utils.data.DataLoader(\n",
    "                    opencv_test_dataset, \n",
    "                    batch_size=batch_size, \n",
    "                    shuffle=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UNET_Binary Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets for training and validation\n",
    "unetb_train_dataset = datasets.ImageFolder(\n",
    "                    root='../input/Classification/UNET_Binary/train', \n",
    "                    transform=train_transform\n",
    "                    )\n",
    "unetb_val_dataset = datasets.ImageFolder(\n",
    "                    root='../input/Classification/UNET_Binary/val', \n",
    "                    transform=eval_transform\n",
    "                    )\n",
    "unetb_test_dataset = datasets.ImageFolder(\n",
    "                    root='../input/Classification/UNET_Binary/test', \n",
    "                    transform=eval_transform\n",
    "                    )\n",
    "\n",
    "# Create PyTorch dataloaders for training and validation\n",
    "unetb_train_dataloader = torch.utils.data.DataLoader(\n",
    "                    unetb_train_dataset,\n",
    "                    batch_size=batch_size, \n",
    "                    shuffle=True\n",
    "                    )\n",
    "unetb_val_dataloader = torch.utils.data.DataLoader(\n",
    "                    unetb_val_dataset, \n",
    "                    batch_size=batch_size, \n",
    "                    shuffle=True\n",
    "                    )\n",
    "unetb_test_dataloader = torch.utils.data.DataLoader(\n",
    "                    unetb_test_dataset, \n",
    "                    batch_size=batch_size, \n",
    "                    shuffle=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UNET_Multiclass Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets for training and validation\n",
    "unetm_train_dataset = datasets.ImageFolder(\n",
    "                    root='../input/Classification/UNET_Multiclass/train', \n",
    "                    transform=train_transform\n",
    "                    )\n",
    "unetm_val_dataset = datasets.ImageFolder(\n",
    "                    root='../input/Classification/UNET_Multiclass/val', \n",
    "                    transform=eval_transform\n",
    "                    )\n",
    "unetm_test_dataset = datasets.ImageFolder(\n",
    "                    root='../input/Classification/UNET_Multiclass/test', \n",
    "                    transform=eval_transform\n",
    "                    )\n",
    "\n",
    "# Create PyTorch dataloaders for training and validation\n",
    "unetm_train_dataloader = torch.utils.data.DataLoader(\n",
    "                    unetm_train_dataset,\n",
    "                    batch_size=batch_size, \n",
    "                    shuffle=True\n",
    "                    )\n",
    "unetm_val_dataloader = torch.utils.data.DataLoader(\n",
    "                    unetm_val_dataset, \n",
    "                    batch_size=batch_size, \n",
    "                    shuffle=True\n",
    "                    )\n",
    "unetm_test_dataloader = torch.utils.data.DataLoader(\n",
    "                    unetm_test_dataset, \n",
    "                    batch_size=batch_size, \n",
    "                    shuffle=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_img(dataloader, class_list: list):\n",
    "    \"\"\"\n",
    "    Function to visualize the first 9 images of the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataloader (DataLoader): PyTorch DataLoader object containing the dataset to visualize.\n",
    "        class_list (list): List of class labels.\n",
    "    \"\"\"\n",
    "    #Get the first batch of images and labels\n",
    "    train_images, train_labels = next(iter(dataloader))\n",
    "    batch_size = train_images.size(0)  # Get the batch size\n",
    "\n",
    "    #Print the shape of the batch\n",
    "    print(f\"Images batch shape: {train_images.size()}\")\n",
    "    print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "    #Create a 3x3 grid for visualization\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(9, 9))\n",
    "\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            #Get the index of the image in the batch\n",
    "            index = i * 3 + j\n",
    "\n",
    "            if index < batch_size:\n",
    "                #Prepare image to print\n",
    "                img = train_images[index].squeeze().numpy().transpose((1, 2, 0))\n",
    "                label = train_labels[index].item()\n",
    "\n",
    "                #Plot the image\n",
    "                axes[i, j].imshow(img)\n",
    "                axes[i, j].axis('off')\n",
    "                axes[i, j].set_title(f'Label: {label}, {class_list[label]}', loc='left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_img(ben_train_dataloader, ben_train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_img(opencv_train_dataloader, opencv_train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_img(unetb_train_dataloader, unetb_train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_img(unetm_train_dataloader, unetm_train_dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Eval Functions for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dr_class_weights(dataset):\n",
    "    \"\"\"\n",
    "    Function to calculate class weights\n",
    "    Class Weights of i = Total Num of Samples / Total Num of samples of Class i * Num of classes\n",
    "\n",
    "    We calculate the average of samples per class if it was equally distributed and then calculate the class weights based on the difference of actual vs ideal\n",
    "\n",
    "    Args:\n",
    "        dataset: train dataset\n",
    "\n",
    "    Returns:\n",
    "        weights (np.ndarray): array of size n_classes with the weights of each class in each index\n",
    "    \"\"\"\n",
    "    # Counting the number of samples in each class\n",
    "    class_counts = np.bincount(dataset.targets)\n",
    "    total_samples = sum(class_counts)\n",
    "    num_classes = len(class_counts)\n",
    "    \n",
    "    # Calculating class weights inversely proportional to the number of samples in each class\n",
    "    weights = total_samples / (num_classes * class_counts)\n",
    "\n",
    "    print(\"Class Weights:\", weights)\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, \n",
    "         criterion, \n",
    "         img_size:tuple,\n",
    "         val_dataloader, \n",
    "         device='cuda'):\n",
    "    \"\"\"\n",
    "    Evaluation function for finetuning CNN models with a model object,\n",
    "    incorporating average sensitivity for a multiclass problem.\n",
    "\n",
    "    Sensitivity function: True Positives / (True Positives + False Negatives)\n",
    "\n",
    "    Args:\n",
    "        model: model to be trained\n",
    "        criterion: loss function\n",
    "        img_size (tuple): image size of dataset for model. All inputs will be resized to image size\n",
    "        val_dataloader: val / test dataloader\n",
    "        device (str, optional): 'cpu' or 'cuda', defaults to cuda.\n",
    "\n",
    "    Returns:\n",
    "        val_loss: float of the average val loss.\n",
    "        val_accuracy: float of the accuracy.\n",
    "        val_sensitivity: float of the average sensitivity across all classes.\n",
    "    \"\"\"\n",
    "\n",
    "    model = model.to(device)\n",
    "    \n",
    "    #set model to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    #variables \n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    n_classes = 5\n",
    "    true_positives = [0] * n_classes #stores num of true positives per class\n",
    "    actual_positives = [0] * n_classes #stores total number of positives per class\n",
    "    total_sensitivity = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in val_dataloader:\n",
    "            batch_sensitivity = 0\n",
    "\n",
    "            #resize image with bilinear, same as torchvision.transforms.Resize()\n",
    "            image = torch.nn.functional.interpolate(image, size=img_size, mode='bilinear') \n",
    "            image, label = image.to(device), label.to(device)\n",
    "\n",
    "            outputs = model(image)  #predict label\n",
    "            loss = criterion(outputs, label)  #calculate loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1) #get prediction\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "\n",
    "            for i in range(n_classes):\n",
    "                true_positives[i] += ((predicted == i) & (label == i)).sum().item() #true positives\n",
    "                actual_positives[i] += (label == i).sum().item() #true positives + false negatives\n",
    "            \n",
    "                if (label == i).sum().item() > 0:\n",
    "                    batch_sensitivity += true_positives[i] / actual_positives[i]\n",
    "\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "            total_sensitivity += batch_sensitivity/n_classes #average sensitivity for batch\n",
    "\n",
    "    # Calculate accuracy, avg loss, and avg sensitivity\n",
    "    accuracy = (correct / total) * 100\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    avg_sensitivity = total_sensitivity/len(val_dataloader)\n",
    "    \n",
    "    return avg_val_loss, accuracy, avg_sensitivity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, \n",
    "          criterion, \n",
    "          optimiser, \n",
    "          img_size:tuple,\n",
    "          train_dataloader, \n",
    "          val_dataloader=None, \n",
    "          saving_metric:str='sensitivity',\n",
    "          num_epochs:int=25, \n",
    "          device:str='cuda', \n",
    "          model_name:str=None):\n",
    "    \"\"\"\n",
    "    Training Function to train model\n",
    "    Runs validation for each epoch to calculate: Validation Loss, Validation Accuracy, Validation Sensitivity\n",
    "    Best and last model will be saved to ../models/cnn under {model_name}_best.pt and {model_name}_last.pt\n",
    "    \n",
    "    Args:\n",
    "        model: model to be trained\n",
    "        criterion: loss function\n",
    "        optimiser: optimiser chosen\n",
    "        img_size (tuple): image size of dataset for model. All inputs will be resized to image size\n",
    "        train_dataloader: train dataloader\n",
    "        val_dataloader (optional): val dataloader, if None no validation will be calculated. Defaults to None.\n",
    "        saving_metric (str, optional): saving metrics for best model, either \"loss\", \"accuracy\", or \"sensitivity\". Defaults to 'sensitivity'.\n",
    "        num_epochs (int, optional): number of training epochs. Defaults to 25.\n",
    "        device (str, optional): cuda or cpu. Defaults to 'cuda'.\n",
    "        model_name (str, optional): model name to be saved, if None no model will be saved. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        results_dataframe: dataframe of [model, train_loss, val_loss, val_accuracy, val_sensitivity] where each row is each epoch\n",
    "    \"\"\"\n",
    "\n",
    "    if saving_metric not in [\"loss\", \"accuracy\", \"sensitivity\"]:\n",
    "        raise Exception(\"Invalid saving metrics found, please only use loss, accuracy or sensitivity\")\n",
    "\n",
    "    #initialising results container\n",
    "    results = pd.DataFrame(columns=[\"Model\", \"train loss\", \"val loss\", \"val accuracy\", \"val sensitivity\"])\n",
    "    \n",
    "    #placeholders\n",
    "    val_loss = ''\n",
    "    val_accuracy = ''\n",
    "    val_sensitivity = ''\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        #initialising training\n",
    "        model.train()\n",
    "        training_loss = 0.0\n",
    "\n",
    "        for image, label in tqdm(train_dataloader):\n",
    "            \n",
    "            #resize image with bilinear, same as torchvision.transforms.Resize()\n",
    "            image = torch.nn.functional.interpolate(image, size=img_size, mode='bilinear') \n",
    "            image, label = image.to(device), label.to(device)\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            outputs = model(image)\n",
    "            #calculate loss and train model\n",
    "            loss = criterion(outputs, label)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            training_loss += loss.item() #update the training loss\n",
    "\n",
    "        epoch_loss = training_loss / len(train_dataloader) #calculate training loss in epoch\n",
    "        print(f\"Epoch {epoch+1} completed, training loss: {epoch_loss}\")\n",
    "\n",
    "        #validation\n",
    "        if val_dataloader is not None:\n",
    "            model.eval()  #set model to evaluate mode\n",
    "            val_loss, val_accuracy,  val_sensitivity = eval(model=model, \n",
    "                                                               criterion=criterion,\n",
    "                                                                val_dataloader=val_dataloader, \n",
    "                                                                img_size = img_size,\n",
    "                                                                device=device) \n",
    "            \n",
    "            print(f\"Validation loss: {val_loss}, Validation Accuracy: {val_accuracy:.2f}, Validation Sensitivty: {val_sensitivity:2f}\")\n",
    "            \n",
    "\n",
    "            if saving_metric == 'loss' and len(results) > 0 and val_loss < min(results['val loss'].to_list()):\n",
    "                torch.save(model, f'../models/cnn/{model_name}_best.pt')\n",
    "                print(\"Best model saved\")\n",
    "\n",
    "            elif saving_metric == 'accuracy' and len(results) > 0 and val_accuracy > max(results['val accuracy'].to_list()):\n",
    "                torch.save(model, f'../models/cnn/{model_name}_best.pt')\n",
    "                print(\"Best model saved\")\n",
    "\n",
    "            elif saving_metric == 'sensitivity' and len(results) > 0 and val_sensitivity > max(results['val sensitivity'].to_list()):\n",
    "                torch.save(model, f'../models/cnn/{model_name}_best.pt')\n",
    "                print(\"Best model saved\")\n",
    "            \n",
    "        #updating results\n",
    "        results.loc[len(results)] = [model_name, epoch_loss, val_loss, val_accuracy, val_sensitivity]\n",
    "\n",
    "\n",
    "    #save the last model\n",
    "    if model_name is not None:\n",
    "        torch.save(model, f'../models/cnn/{model_name}_last.pt')\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Eval Functions for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_outputs(model_list:list, dataloader, device:str='cuda'):\n",
    "    \"\"\"\n",
    "    Function to generate the outputs of the model from dataloader and add them to a dictionary\n",
    "    Expects model_list to be either [[]] or [[], []] \n",
    "\n",
    "    Args:\n",
    "        model_list (list): nested list of either [[[model, model_name, img_size], [model, model_name, img_size]]] or [[[model, model_name, img_size], [model, model_name, img_size]], [[model, model_name, img_size], [model, model_name, img_size]]]\n",
    "        dataloader: dataloader to run outputs for\n",
    "        device (str, optional): cuda or cpu. Defaults to 'cuda'.\n",
    "\n",
    "    Returns:\n",
    "        model_outputs (dict): Dict in the format of {model_name: [output]}\n",
    "        true_labels (list): list of labels for each output\n",
    "    \"\"\"\n",
    "    model_outputs = {}\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, label in tqdm(dataloader):\n",
    "            #get model output and update in dictionary\n",
    "            for model, model_name, img_size in model_list[0]:\n",
    "                model.eval()\n",
    "                images = torch.nn.functional.interpolate(images, img_size, mode='bilinear').to(device)\n",
    "                output = model(images)\n",
    "                existing_results = model_outputs.get(model_name, [])\n",
    "                existing_results.append(output)\n",
    "                model_outputs[model_name] = existing_results\n",
    "\n",
    "                #for nested list, run for second batch of models\n",
    "                if len(model_list) > 1:\n",
    "                    for model, model_name, img_size in model_list[1]:\n",
    "                        model.eval()\n",
    "                        images = torch.nn.functional.interpolate(images, img_size, mode='bilinear').to(device)\n",
    "                        output = model(images)\n",
    "                        existing_results = model_outputs.get(model_name, [])\n",
    "                        existing_results.append(output)\n",
    "                        model_outputs[model_name] = existing_results\n",
    "\n",
    "            true_labels.extend(label.item())\n",
    "\n",
    "    for name, results in model_outputs.values():\n",
    "        formatted_results = torch.cat(results, dim=0) #removing the batchsize of 1 by flattening it\n",
    "        model_outputs[name] = formatted_results\n",
    "\n",
    "    return model_outputs, true_labels\n",
    "\n",
    "\n",
    "\n",
    "def train_svm(model_list:list, train_dataloader, test_dataloader=None, device:str='cuda', save_model:bool=False):\n",
    "    \"\"\"\n",
    "    Function to train a svm\n",
    "    Model list can either be nested list of length 1: [[[model, model_name, img_size], [model, model_name, img_size], ...]]\n",
    "    - this tells the function that any pair of models can be chosen in this list\n",
    "\n",
    "    Model list can also be nested with length 2: [[[model, model_name, img_size], [model, model_name, img_size]], [[model, model_name, img_size], [model, model_name, img_size]]]\n",
    "    - this tells the function that each pair must contain 1 model from each list, no picking from the same list\n",
    "\n",
    "    Args:\n",
    "        model_list (list): Either a nested list of length 1 or 2\n",
    "        train_dataloader: train dataloader to run outputs for\n",
    "        test_dataloader (optional): test dataloader to run evaluation on, if None no evaluation will be done. Defaults to None.\n",
    "        device (str, optional): cuda or cpu. Defaults to 'cuda'.\n",
    "        save_model (bool, optional): boolean to save every svm model. Defaults to False.\n",
    "\n",
    "    Raises:\n",
    "        Exception: if model_list length is not 1 or 2\n",
    "\n",
    "    Returns:\n",
    "        recall_result (pd.DataFrame) : dataframe containing recall results in the format of [model1name, model2name, recallscore]\n",
    "    \"\"\"\n",
    "    if len(model_list) != 1 or len(model_list) != 2:\n",
    "        raise Exception(\"Currently only accepting nested list of length 1 or 2\")\n",
    "    \n",
    "    recall_result = pd.DataFrame(columns=[\"Model1\", \"Model2\", \"Recall\"])\n",
    "\n",
    "    #generating model outputs (dataset for svm)\n",
    "    print(f\"---- Generating training data ----\")\n",
    "    model_output, true_labels = generate_model_outputs(model_list, train_dataloader, device)\n",
    "    print(f\"---- Generating testing data ----\")\n",
    "    if test_dataloader != None: \n",
    "        test_model_output, test_true_labels = generate_model_outputs(model_list, test_dataloader, device)\n",
    "\n",
    "    #training svm based on combination\n",
    "    if len(model_list) == 1:\n",
    "        #can combine with ANY model in the list\n",
    "        for i in range(len(model_list[0])):\n",
    "            for j in range(i + 1, len(model_list[0])): \n",
    "                model_name_1 = model_list[0][i][1]\n",
    "                model_name_2 = model_list[0][j][1]\n",
    "\n",
    "                #concat model outputs to form train data\n",
    "                train_data = torch.cat((model_output[model_name_1], model_output[model_name_2]), dim=1).cpu().detach().numpy()\n",
    "                train_labels = np.array(true_labels)\n",
    "                \n",
    "                #initialise and train model\n",
    "                svm_classifier = SVC(decision_function_shape='ovo')\n",
    "                svm_classifier.fit(train_data, train_labels)\n",
    "\n",
    "                #save model\n",
    "                if save_model:\n",
    "                    with open(f'../results/svm/{model_name_1}_{model_name_2}_svm.pkl', 'wb') as f:\n",
    "                        pickle.dump(f)\n",
    "\n",
    "                #concat test model outputs to form test data\n",
    "                test_data = torch.cat((test_model_output[model_name_1], test_model_output[model_name_2]), dim=1).cpu().detach().numpy()\n",
    "                test_labels = np.array(test_true_labels)\n",
    "\n",
    "                #evaluate recall\n",
    "                predictions = svm_classifier.predict(test_data)\n",
    "                recall = recall_score(test_labels, predictions, average='weighted')\n",
    "\n",
    "                print(f\"{model_name_1}, {model_name_2} recall: {recall}\")\n",
    "                recall_result.loc[len(recall_result)] = [model_name_1, model_name_2, recall]\n",
    "\n",
    "\n",
    "    elif len(model_list) == 2:\n",
    "        #models in list 1 can only be paired with models in list 2\n",
    "        for i in range(len(model_list[0])):\n",
    "            for j in range(len(model_list[1])):\n",
    "                model_name_1 = model_list[0][i][1]\n",
    "                model_name_2 = model_list[0][j][1]\n",
    "\n",
    "                #concat model outputs to form train data\n",
    "                train_data = torch.cat((model_output[model_name_1], model_output[model_name_2]), dim=1).cpu().detach().numpy()\n",
    "                train_labels = np.array(true_labels)\n",
    "                \n",
    "                #initialise and train model\n",
    "                svm_classifier = SVC(decision_function_shape='ovo')\n",
    "                svm_classifier.fit(train_data, train_labels)\n",
    "\n",
    "                #save model\n",
    "                if save_model:\n",
    "                    with open(f'../results/svm/{model_name_1}_{model_name_2}_svm.pkl', 'wb') as f:\n",
    "                        pickle.dump(f)\n",
    "\n",
    "                #concat test model outputs to form test data\n",
    "                test_data = torch.cat((test_model_output[model_name_1], test_model_output[model_name_2]), dim=1).cpu().detach().numpy()\n",
    "                test_labels = np.array(test_true_labels)\n",
    "\n",
    "                #evaluate recall\n",
    "                predictions = svm_classifier.predict(test_data)\n",
    "                recall = recall_score(test_labels, predictions, average='weighted')\n",
    "\n",
    "                print(f\"{model_name_1}, {model_name_2} recall: {recall}\")\n",
    "                recall_result.loc[len(recall_result)] = [model_name_1, model_name_2, recall]\n",
    "\n",
    "    return recall_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreTrainedCNNModels(torch.nn.Module):\n",
    "    def __init__(self, model_type:str, num_first_unfreeze:int, num_last_unfreeze:int, num_class:int):\n",
    "        super(PreTrainedCNNModels, self).__init__()\n",
    "        \"\"\"\n",
    "        Class that contains InceptionV3, Resnet50, Resnet152, EfficientNet, DenseNet, VGG16, MaxVit fine tuned models\n",
    "\n",
    "        Args:\n",
    "            model_type (str): Determines which pre-trained models to use\n",
    "                              Must be: InceptionV3, Resnet50, Resnet152, EfficientNet, DenseNet, VGG16, MaxVit\n",
    "            num_first_unfreeze (int): Number of first layers to unfreeze and finetune\n",
    "            num_last_unfreeze (int): Number of layers to unfreeze and finetune\n",
    "            num_class (int): Number of output classes for the classification\n",
    "        \"\"\"\n",
    "        #selecting model type\n",
    "        if model_type == 'InceptionV3':\n",
    "            self.model = models.inception_v3(weights=models.Inception_V3_Weights.DEFAULT)\n",
    "            self.model.aux_logits = False\n",
    "\n",
    "        elif model_type == 'Resnet50':\n",
    "            self.model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "\n",
    "        elif model_type == 'Resnet152':\n",
    "            self.model = models.resnet152(weights=models.ResNet152_Weights.DEFAULT)\n",
    "\n",
    "        elif model_type == 'EfficientNet':\n",
    "            self.model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.DEFAULT)\n",
    "\n",
    "        elif model_type == 'DenseNet':\n",
    "            self.model = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n",
    "        \n",
    "        elif model_type == 'VGG16':\n",
    "            self.model = models.vgg16_bn(weights=models.VGG16_BN_Weights.DEFAULT)\n",
    "\n",
    "        elif model_type == 'MaxVit':\n",
    "            self.model = models.maxvit_t(weights=models.MaxVit_T_Weights.DEFAULT)\n",
    "        \n",
    "        else:\n",
    "            raise Exception(\"Invalid model type chosen. Please select one of the following\\n[InceptionV3, Resnet50, Resnet152, EfficientNet, DenseNet, VGG16, MaxVit]\")\n",
    "\n",
    "        \n",
    "        #modifying final layer\n",
    "        if model_type in ['InceptionV3', 'Resnet50', 'Resnet152']:\n",
    "            self.model.fc = torch.nn.Linear(self.model.fc.in_features, num_class)\n",
    "\n",
    "        elif model_type == 'DenseNet':\n",
    "            self.model.classifier = torch.nn.Linear(self.model.classifier.in_features, num_class)\n",
    "\n",
    "        else:\n",
    "            self.model.classifier[-1] = torch.nn.Linear(self.model.classifier[-1].in_features, num_class)\n",
    "\n",
    "\n",
    "        model_paramteres = list(self.model.parameters())\n",
    "        #unfreeze last num_last_unfreeze layers\n",
    "        for param in model_paramteres[-num_last_unfreeze:]:\n",
    "            param.requires_grad = True\n",
    "\n",
    "        #unfreeze first num_first_unfreeze layers \n",
    "        for param in model_paramteres[:num_first_unfreeze]:\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        #freeze rest of the layers\n",
    "        for param in model_paramteres[num_first_unfreeze:-num_last_unfreeze]:\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, images):\n",
    "        return self.model(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training all model with equal class weights with unfrozen last linear layer\n",
    "\n",
    "Training all model with Cross Entropy Loss without any class weights on Ben Graham dataset\n",
    "\n",
    "Note: We have to put 2 as num_last_unfreeze due to the fact that the lineaer layer has bias and weights and is thus considered as 2 layers in the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "models_list = [['InceptionV3', (299,299)],\n",
    "               ['Resnet50', (224,224)],\n",
    "               ['Resnet152',(224,224)],\n",
    "               ['EfficientNet',(224,224)],\n",
    "               ['DenseNet',(224,224)],\n",
    "               ['VGG16',(224,224)],\n",
    "               ['MaxVit', (224,224)]]\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 50\n",
    "\n",
    "all_results = pd.DataFrame(columns=[\"Model\", \"train loss\", \"val loss\", \"val accuracy\", \"val sensitivity\"])\n",
    "\n",
    "for model_name, img_size in models_list:\n",
    "\n",
    "    #clear cuda memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    #reinitialise model, loss and optimizer\n",
    "    model = PreTrainedCNNModels(model_name, 0, 2, len(ben_train_dataset.classes)).to(device)\n",
    "    criterion=torch.nn.CrossEntropyLoss() #equal class weights\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "    print(f\"-------------Training {model_name}------------\")\n",
    "    model_result = train(model,\n",
    "                         criterion,\n",
    "                         optimizer,\n",
    "                         img_size,\n",
    "                         ben_train_dataloader,\n",
    "                         ben_val_dataloader,\n",
    "                         'sensitivity',\n",
    "                         num_epochs,\n",
    "                         device,\n",
    "                         f\"{model_name}_0_2_base\")\n",
    "    \n",
    "    del model #clear cuda memory\n",
    "    \n",
    "    all_results =  pd.concat([all_results, model_result])\n",
    "\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "models_list = [['InceptionV3', (299,299)],\n",
    "               ['Resnet50', (224,224)],\n",
    "               ['Resnet152',(224,224)],\n",
    "               ['EfficientNet',(224,224)],\n",
    "               ['DenseNet',(224,224)],\n",
    "               ['VGG16',(224,224)],\n",
    "               ['MaxVit', (224,224)]]\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for model_name, img_size in models_list:\n",
    "\n",
    "    #clear cuda memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    best_model = torch.load(f'../models/cnn/{model_name}_0_2_base_best.pt')\n",
    "    last_model = torch.load(f'../model/cnn/{model_name}_0_2_base_last.pt')\n",
    "    criterion=torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    print(f\"-------------Evaluating {model_name}------------\")\n",
    "    #evaluating the best model\n",
    "    loss, accuracy, sensitivity = eval(best_model,\n",
    "                                        criterion,\n",
    "                                        img_size,\n",
    "                                        ben_test_dataloader,\n",
    "                                        device)\n",
    "    print(f\"Best Model - Test Loss: {loss}, Test Accuracy: {accuracy}, Test Sensitivity: {sensitivity}\")\n",
    "    #evaluating the last model    \n",
    "    loss, accuracy, sensitivity = eval(last_model,\n",
    "                                        criterion,\n",
    "                                        img_size,\n",
    "                                        ben_test_dataloader,\n",
    "                                        device)\n",
    "    print(f\"Last Model - Test Loss: {loss}, Test Accuracy: {accuracy}, Test Sensitivity: {sensitivity}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training all model with calculated weighted class with unfrozen last linear layer\n",
    "\n",
    "Training all model with Cross Entropy Loss with calculated class weights on Ben Graham dataset\n",
    "\n",
    "Class Weights of i = Total Num of Samples / Total Num of samples of Class i * Num of classes\n",
    "\n",
    "We calculate the average of samples per class if it was equally distributed and then calculate the class weights based on the difference of actual vs ideal\n",
    "\n",
    "<br/>\n",
    "Note: We have to put 2 as num_last_unfreeze due to the fact that the lineaer layer has bias and weights and is thus considered as 2 layers in the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = calculate_dr_class_weights(ben_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "models_list = [['InceptionV3', (299,299)],\n",
    "               ['Resnet50', (224,224)],\n",
    "               ['Resnet152',(224,224)],\n",
    "               ['EfficientNet',(224,224)],\n",
    "               ['DenseNet',(224,224)],\n",
    "               ['VGG16',(224,224)],\n",
    "               ['MaxVit', (224,224)]]\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 50\n",
    "\n",
    "all_results = pd.DataFrame(columns=[\"Model\", \"train loss\", \"val loss\", \"val accuracy\", \"val sensitivity\"])\n",
    "\n",
    "for model_name, img_size in models_list:\n",
    "\n",
    "    #clear cuda memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    #reinitialise model, loss and optimizer\n",
    "    model = PreTrainedCNNModels(model_name, 0, 2, len(ben_train_dataset.classes)).to(device)\n",
    "    criterion=torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(device)) #calculated weights\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "    print(f\"-------------Training {model_name}------------\")\n",
    "    model_result = train(model,\n",
    "                         criterion,\n",
    "                         optimizer,\n",
    "                         img_size,\n",
    "                         ben_train_dataloader,\n",
    "                         ben_val_dataloader,\n",
    "                         'sensitivity',\n",
    "                         num_epochs,\n",
    "                         device,\n",
    "                         f\"{model_name}_0_2_weighted\")\n",
    "    \n",
    "    del model #clear cuda memory\n",
    "    \n",
    "    all_results =  pd.concat([all_results, model_result])\n",
    "\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference\n",
    "models_list = [['InceptionV3', (299,299)],\n",
    "               ['Resnet50', (224,224)],\n",
    "               ['Resnet152',(224,224)],\n",
    "               ['EfficientNet',(224,224)],\n",
    "               ['DenseNet',(224,224)],\n",
    "               ['VGG16',(224,224)],\n",
    "               ['MaxVit', (224,224)]]\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for model_name, img_size in models_list:\n",
    "\n",
    "    #clear cuda memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    best_model = torch.load(f'../models/cnn/{model_name}_0_2_weighted_best.pt')\n",
    "    last_model = torch.load(f'../models/cnn/{model_name}_0_2_weighted_last.pt')\n",
    "    criterion=torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    print(f\"-------------Evaluating {model_name}------------\")\n",
    "    #evaluating the best model\n",
    "    loss, accuracy, sensitivity = eval(best_model,\n",
    "                                        criterion,\n",
    "                                        img_size,\n",
    "                                        ben_test_dataloader,\n",
    "                                        device)\n",
    "    print(f\"Best Model - Test Loss: {loss}, Test Accuracy: {accuracy}, Test Sensitivity: {sensitivity}\")\n",
    "    #evaluating the last model    \n",
    "    loss, accuracy, sensitivity = eval(last_model,\n",
    "                                        criterion,\n",
    "                                        img_size,\n",
    "                                        ben_test_dataloader,\n",
    "                                        device)\n",
    "    print(f\"Last Model - Test Loss: {loss}, Test Accuracy: {accuracy}, Test Sensitivity: {sensitivity}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training all model with custom weighted class with unfrozen last linear layer\n",
    "\n",
    "Training all model with Cross Entropy Loss with custom class weights [1, 6, 1, 10, 3] on Ben Graham dataset\n",
    "\n",
    "\n",
    "Note: We have to put 2 as num_last_unfreeze due to the fact that the lineaer layer has bias and weights and is thus considered as 2 layers in the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "models_list = [['InceptionV3', (299,299)],\n",
    "               ['Resnet50', (224,224)],\n",
    "               ['Resnet152',(224,224)],\n",
    "               ['EfficientNet',(224,224)],\n",
    "               ['DenseNet',(224,224)],\n",
    "               ['VGG16',(224,224)],\n",
    "               ['MaxVit', (224,224)]]\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 50\n",
    "\n",
    "all_results = pd.DataFrame(columns=[\"Model\", \"train loss\", \"val loss\", \"val accuracy\", \"val sensitivity\"])\n",
    "\n",
    "for model_name, img_size in models_list:\n",
    "\n",
    "    #clear cuda memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    #reinitialise model, loss and optimizer\n",
    "    model = PreTrainedCNNModels(model_name, 0, 2, len(ben_train_dataset.classes)).to(device)\n",
    "    criterion=torch.nn.CrossEntropyLoss(weight=torch.tensor([1, 6, 1, 10, 3], dtype=torch.float32).to(device)) #custom weights\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "    print(f\"-------------Training {model_name}------------\")\n",
    "    model_result = train(model,\n",
    "                         criterion,\n",
    "                         optimizer,\n",
    "                         img_size,\n",
    "                         ben_train_dataloader,\n",
    "                         ben_val_dataloader,\n",
    "                         'sensitivity',\n",
    "                         num_epochs,\n",
    "                         device,\n",
    "                         f\"{model_name}_0_2_custom_weighted\")\n",
    "    \n",
    "    del model #clear cuda memory\n",
    "    \n",
    "    all_results =  pd.concat([all_results, model_result])\n",
    "    \n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = [['InceptionV3', (299,299)],\n",
    "               ['Resnet50', (224,224)],\n",
    "               ['Resnet152',(224,224)],\n",
    "               ['EfficientNet',(224,224)],\n",
    "               ['DenseNet',(224,224)],\n",
    "               ['VGG16',(224,224)],\n",
    "               ['MaxVit', (224,224)]]\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for model_name, img_size in models_list:\n",
    "\n",
    "    #clear cuda memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    best_model = torch.load(f'../models/cnn/{model_name}_0_2_custom_weighted_best.pt')\n",
    "    last_model = torch.load(f'../models/cnn/{model_name}_0_2_custom_weighted_last.pt')\n",
    "    criterion=torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    print(f\"-------------Evaluating {model_name}------------\")\n",
    "    #evaluating the best model\n",
    "    loss, accuracy, sensitivity = eval(best_model,\n",
    "                                        criterion,\n",
    "                                        img_size,\n",
    "                                        ben_test_dataloader,\n",
    "                                        device)\n",
    "    print(f\"Best Model - Test Loss: {loss}, Test Accuracy: {accuracy}, Test Sensitivity: {sensitivity}\")\n",
    "    #evaluating the last model    \n",
    "    loss, accuracy, sensitivity = eval(last_model,\n",
    "                                        criterion,\n",
    "                                        img_size,\n",
    "                                        ben_test_dataloader,\n",
    "                                        device)\n",
    "    print(f\"Last Model - Test Loss: {loss}, Test Accuracy: {accuracy}, Test Sensitivity: {sensitivity}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training all model with calculated weighted class with 8 last unfrozen layers before training with SVM\n",
    "\n",
    "Training all model with Cross Entropy Loss with calculated class weights on Ben Graham dataset with last 8 children layers unfreezed\n",
    "\n",
    "This helps us decide which model shall we be focusing on for layer exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Classification training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = calculate_dr_class_weights(ben_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "models_list = [['InceptionV3', (299,299)],\n",
    "               ['Resnet50', (224,224)],\n",
    "               ['Resnet152',(224,224)],\n",
    "               ['EfficientNet',(224,224)],\n",
    "               ['DenseNet',(224,224)],\n",
    "               ['VGG16',(224,224)],\n",
    "               ['MaxVit', (224,224)]]\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 30\n",
    "\n",
    "all_results = pd.DataFrame(columns=[\"Model\", \"train loss\", \"val loss\", \"val accuracy\", \"val sensitivity\"])\n",
    "\n",
    "for model_name, img_size in models_list:\n",
    "\n",
    "    #clear cuda memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    #reinitialise model, loss and optimizer\n",
    "    model = PreTrainedCNNModels(model_name, 0, 8, len(ben_train_dataset.classes)).to(device)\n",
    "    criterion=torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(device))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "    print(f\"-------------Training {model_name}------------\")\n",
    "    model_result = train(model,\n",
    "                         criterion,\n",
    "                         optimizer,\n",
    "                         img_size,\n",
    "                         ben_train_dataloader,\n",
    "                         ben_val_dataloader,\n",
    "                         'sensitivity',\n",
    "                         num_epochs,\n",
    "                         device,\n",
    "                         f\"{model_name}_0_8_weighted\")\n",
    "    \n",
    "    del model #clear cuda memory\n",
    "\n",
    "    all_results =  pd.concat([all_results, model_result])\n",
    "\n",
    "\n",
    "all_results\n",
    "all_results.to_csv(\"../results/cnn_8_weighted.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference\n",
    "models_list = [['InceptionV3', (299,299)],\n",
    "               ['Resnet50', (224,224)],\n",
    "               ['Resnet152',(224,224)],\n",
    "               ['EfficientNet',(224,224)],\n",
    "               ['DenseNet',(224,224)],\n",
    "               ['VGG16',(224,224)],\n",
    "               ['MaxVit', (224,224)]]\n",
    "\n",
    "#Creating SVM model list\n",
    "svm_model_list = []\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for model_name, img_size in models_list:\n",
    "\n",
    "    #clear cuda memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    best_model = torch.load(f'../models/cnn/{model_name}_0_8_weighted_best.pt')\n",
    "    last_model = torch.load(f'../models/cnn/{model_name}_0_8_weighted_last.pt')\n",
    "    criterion=torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    print(f\"-------------Evaluating {model_name}------------\")\n",
    "    #evaluating the best model\n",
    "    loss, accuracy, best_sensitivity = eval(best_model,\n",
    "                                        criterion,\n",
    "                                        img_size,\n",
    "                                        ben_test_dataloader,\n",
    "                                        device)\n",
    "    print(f\"Best Model - Test Loss: {loss}, Test Accuracy: {accuracy}, Test Sensitivity: {best_sensitivity}\")\n",
    "    #evaluating the last model    \n",
    "    loss, accuracy, last_sensitivity = eval(last_model,\n",
    "                                        criterion,\n",
    "                                        img_size,\n",
    "                                        ben_test_dataloader,\n",
    "                                        device)\n",
    "    print(f\"Last Model - Test Loss: {loss}, Test Accuracy: {accuracy}, Test Sensitivity: {last_sensitivity}\\n\")\n",
    "\n",
    "    #adding best model to svm_model_list\n",
    "    if best_sensitivity > last_sensitivity:\n",
    "        svm_model_list.append([best_model, f\"{model_name}_0_8_weighted\", img_size])\n",
    "    \n",
    "    else:\n",
    "        svm_model_list.append([last_model, f\"{model_name}_0_8_weighted\", img_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "recall_results = train_svm([svm_model_list], ben_train_dataloader, ben_test_dataloader, device, True)\n",
    "\n",
    "recall_results.to_csv(\"../results/svm_0_8_weighted.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet and MaxVit Layer Exploration with SVM for Ben Graham Dataset\n",
    "\n",
    "Exploring different numbers of unfrozen layers in DenseNet and MaxViT with Ben Graham dataset before passing it through an SVM\n",
    "\n",
    "The layers explored will be\n",
    "- first 0 layer unfrozen and last layer unfrozen (done earlier)\n",
    "- first 1 layer unfrozen and last layers unfrozen\n",
    "- first 0 layer unfrozen and last 8 layers unfrozen (done earlier)\n",
    "- first 1 layer unfrozen and last 8 layers unfrozen\n",
    "- first 0 layer unfrozen and last 16 layers unfrozen\n",
    "- first 1 layer unfrozen and last 16 layers unfrozen\n",
    "- first 0 layer unfrozen and last 32 layers unfrozen\n",
    "- first 0 layer unfrozen and last 32 layers unfrozen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DenseNet Layer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = calculate_dr_class_weights(ben_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "layer_list = [[1, 2], #ignoring [0,2] amd [0,8] as they are trained already\n",
    "              [1, 8],\n",
    "              [0, 16],\n",
    "              [1, 16],\n",
    "              [0, 32],\n",
    "              [1, 32]]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 30\n",
    "\n",
    "all_results = pd.DataFrame(columns=[\"Model\", \"train loss\", \"val loss\", \"val accuracy\", \"val sensitivity\"])\n",
    "\n",
    "for num_first_unfreeze, num_last_unfreeze in layer_list:\n",
    "\n",
    "    #clear cuda memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    #reinitialise model with correct weights, loss and optimizer\n",
    "    model = PreTrainedCNNModels(\"DenseNet\", num_first_unfreeze, num_last_unfreeze, len(ben_train_dataset.classes)).to(device)\n",
    "    criterion=torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(device))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "    print(f\"-------------Training DenseNet with {num_first_unfreeze} Unfrozen First Layers and {num_last_unfreeze} Unfrozen Last Layers------------\")\n",
    "    model_result = train(model,\n",
    "                         criterion,\n",
    "                         optimizer,\n",
    "                         img_size,\n",
    "                         ben_train_dataloader,\n",
    "                         ben_val_dataloader,\n",
    "                         'sensitivity',\n",
    "                         num_epochs,\n",
    "                         device,\n",
    "                         f\"DenseNet_{num_first_unfreeze}_{num_last_unfreeze}_weighted\")\n",
    "    \n",
    "    all_results =  pd.concat([all_results, model_result])\n",
    "\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "layer_list = [[0, 2],\n",
    "              [1, 2],\n",
    "              [0, 8],\n",
    "              [1, 8],\n",
    "              [0, 16],\n",
    "              [1, 16],\n",
    "              [0, 32],\n",
    "              [1, 32]]\n",
    "\n",
    "#Creating densenet SVM model list\n",
    "densenet_svm_model_list = []\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for num_first_unfreeze, num_last_unfreeze in layer_list:\n",
    "\n",
    "    #clear cuda memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    best_model = torch.load(f'../models/cnn/DenseNet_{num_first_unfreeze}_{num_last_unfreeze}_weighted_best.pt')\n",
    "    last_model = torch.load(f'../models/cnn/DenseNet_{num_first_unfreeze}_{num_last_unfreeze}_weighted_last.pt')\n",
    "    criterion=torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    print(f\"-------------Evaluating DenseNet with {num_first_unfreeze} Unfrozen First Layers and {num_last_unfreeze} Unfrozen Last Layers------------\")\n",
    "    #evaluating the best model\n",
    "    loss, accuracy, best_sensitivity = eval(best_model,\n",
    "                                        criterion,\n",
    "                                        img_size,\n",
    "                                        ben_test_dataloader,\n",
    "                                        device)\n",
    "    print(f\"Best Model - Test Loss: {loss}, Test Accuracy: {accuracy}, Test Sensitivity: {best_sensitivity}\")\n",
    "    #evaluating the last model    \n",
    "    loss, accuracy, last_sensitivity = eval(last_model,\n",
    "                                        criterion,\n",
    "                                        img_size,\n",
    "                                        ben_test_dataloader,\n",
    "                                        device)\n",
    "    print(f\"Last Model - Test Loss: {loss}, Test Accuracy: {accuracy}, Test Sensitivity: {last_sensitivity}\\n\")\n",
    "\n",
    "    #adding best model to svm_model_list\n",
    "    if best_sensitivity > last_sensitivity:\n",
    "        densenet_svm_model_list.append([best_model, f\"DenseNet_{num_first_unfreeze}_{num_last_unfreeze}_weighted\", (224,224)])\n",
    "    \n",
    "    else:\n",
    "        densenet_svm_model_list.append([last_model, f\"DenseNet_{num_first_unfreeze}_{num_last_unfreeze}_weighted\", (224,224)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MaxVit Layer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = calculate_dr_class_weights(ben_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "layer_list = [[1, 2], #ignoring [0,2] amd [0,8] as they are trained already\n",
    "              [1, 8],\n",
    "              [0, 16],\n",
    "              [1, 16],\n",
    "              [0, 32],\n",
    "              [1, 32]]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 30\n",
    "\n",
    "all_results = pd.DataFrame(columns=[\"Model\", \"train loss\", \"val loss\", \"val accuracy\", \"val sensitivity\"])\n",
    "\n",
    "for num_first_unfreeze, num_last_unfreeze in layer_list:\n",
    "\n",
    "    #clear cuda memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    #reinitialise model with correct weights, loss and optimizer\n",
    "    model = PreTrainedCNNModels(\"MaxVit\", num_first_unfreeze, num_last_unfreeze, len(ben_train_dataset.classes)).to(device)\n",
    "    criterion=torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(device))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "    print(f\"-------------Training MaxVit with {num_first_unfreeze} Unfrozen First Layers and {num_last_unfreeze} Unfrozen Last Layers------------\")\n",
    "    model_result = train(model,\n",
    "                         criterion,\n",
    "                         optimizer,\n",
    "                         img_size,\n",
    "                         ben_train_dataloader,\n",
    "                         ben_val_dataloader,\n",
    "                         'sensitivity',\n",
    "                         num_epochs,\n",
    "                         device,\n",
    "                         f\"MaxVit_{num_first_unfreeze}_{num_last_unfreeze}_weighted\")\n",
    "    \n",
    "    all_results =  pd.concat([all_results, model_result])\n",
    "\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "layer_list = [[0, 2],\n",
    "              [1, 2],\n",
    "              [0, 8],\n",
    "              [1, 8],\n",
    "              [0, 16],\n",
    "              [1, 16],\n",
    "              [0, 32],\n",
    "              [1, 32]]\n",
    "\n",
    "#Creating densenet SVM model list\n",
    "maxvit_svm_model_list = []\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for num_first_unfreeze, num_last_unfreeze in layer_list:\n",
    "\n",
    "    #clear cuda memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    best_model = torch.load(f'../models/cnn/MaxVit_{num_first_unfreeze}_{num_last_unfreeze}_weighted_best.pt')\n",
    "    last_model = torch.load(f'../models/cnn/MaxVit_{num_first_unfreeze}_{num_last_unfreeze}_weighted_last.pt')\n",
    "    criterion=torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    print(f\"-------------Evaluating MaxVit with {num_first_unfreeze} Unfrozen First Layers and {num_last_unfreeze} Unfrozen Last Layers------------\")\n",
    "    #evaluating the best model\n",
    "    loss, accuracy, best_sensitivity = eval(best_model,\n",
    "                                        criterion,\n",
    "                                        img_size,\n",
    "                                        ben_test_dataloader,\n",
    "                                        device)\n",
    "    print(f\"Best Model - Test Loss: {loss}, Test Accuracy: {accuracy}, Test Sensitivity: {best_sensitivity}\")\n",
    "    #evaluating the last model    \n",
    "    loss, accuracy, last_sensitivity = eval(last_model,\n",
    "                                        criterion,\n",
    "                                        img_size,\n",
    "                                        ben_test_dataloader,\n",
    "                                        device)\n",
    "    print(f\"Last Model - Test Loss: {loss}, Test Accuracy: {accuracy}, Test Sensitivity: {last_sensitivity}\\n\")\n",
    "\n",
    "    #adding best model to svm_model_list\n",
    "    if best_sensitivity > last_sensitivity:\n",
    "        maxvit_svm_model_list.append([best_model, f\"MaxVit_{num_first_unfreeze}_{num_last_unfreeze}_weighted\", (224,224)])\n",
    "    \n",
    "    else:\n",
    "        maxvit_svm_model_list.append([last_model, f\"MaxVit_{num_first_unfreeze}_{num_last_unfreeze}_weighted\", (224,224)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "recall_results = train_svm([densenet_svm_model_list, maxvit_svm_model_list], ben_train_dataloader, ben_test_dataloader, device, True)\n",
    "\n",
    "recall_results.to_csv(\"../results/densenet_maxvit_ben.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet and MaxVit Layer Exploration with SVM for OPENCV Dataset\n",
    "\n",
    "Exploring different numbers of unfrozen layers in DenseNet and MaxViT with OPENCV dataset before passing it through an SVM\n",
    "\n",
    "The layers explored will be\n",
    "- first 0 layer unfrozen and last layer unfrozen (done earlier)\n",
    "- first 1 layer unfrozen and last layers unfrozen\n",
    "- first 0 layer unfrozen and last 8 layers unfrozen (done earlier)\n",
    "- first 1 layer unfrozen and last 8 layers unfrozen\n",
    "- first 0 layer unfrozen and last 16 layers unfrozen\n",
    "- first 1 layer unfrozen and last 16 layers unfrozen\n",
    "- first 0 layer unfrozen and last 32 layers unfrozen\n",
    "- first 0 layer unfrozen and last 32 layers unfrozen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DenseNet Layer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = calculate_dr_class_weights(opencv_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "layer_list = [[0, 2],\n",
    "              [1, 2],\n",
    "              [0, 8],\n",
    "              [1, 8],\n",
    "              [0, 16],\n",
    "              [1, 16],\n",
    "              [0, 32],\n",
    "              [1, 32]]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 30\n",
    "\n",
    "all_results = pd.DataFrame(columns=[\"Model\", \"train loss\", \"val loss\", \"val accuracy\", \"val sensitivity\"])\n",
    "\n",
    "for num_first_unfreeze, num_last_unfreeze in layer_list:\n",
    "\n",
    "    #clear cuda memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    #reinitialise model with correct weights, loss and optimizer\n",
    "    model = PreTrainedCNNModels(\"DenseNet\", num_first_unfreeze, num_last_unfreeze, len(opencv_train_dataset.classes)).to(device)\n",
    "    criterion=torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(device))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "    print(f\"-------------Training DenseNet with {num_first_unfreeze} Unfrozen First Layers and {num_last_unfreeze} Unfrozen Last Layers------------\")\n",
    "    model_result = train(model,\n",
    "                         criterion,\n",
    "                         optimizer,\n",
    "                         img_size,\n",
    "                         opencv_train_dataloader,\n",
    "                         opencv_val_dataloader,\n",
    "                         'sensitivity',\n",
    "                         num_epochs,\n",
    "                         device,\n",
    "                         f\"DenseNet_opencv_{num_first_unfreeze}_{num_last_unfreeze}_weighted\")\n",
    "    \n",
    "    all_results =  pd.concat([all_results, model_result])\n",
    "\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "layer_list = [[0, 2],\n",
    "              [1, 2],\n",
    "              [0, 8],\n",
    "              [1, 8],\n",
    "              [0, 16],\n",
    "              [1, 16],\n",
    "              [0, 32],\n",
    "              [1, 32]]\n",
    "\n",
    "#Creating densenet SVM model list\n",
    "densenet_svm_model_list = []\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for num_first_unfreeze, num_last_unfreeze in layer_list:\n",
    "\n",
    "    #clear cuda memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    best_model = torch.load(f'../models/cnn/DenseNet_opencv_{num_first_unfreeze}_{num_last_unfreeze}_weighted_best.pt')\n",
    "    last_model = torch.load(f'../models/cnn/DenseNet_opencv_{num_first_unfreeze}_{num_last_unfreeze}_weighted_last.pt')\n",
    "    criterion=torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    print(f\"-------------Evaluating DenseNet with {num_first_unfreeze} Unfrozen First Layers and {num_last_unfreeze} Unfrozen Last Layers------------\")\n",
    "    #evaluating the best model\n",
    "    loss, accuracy, best_sensitivity = eval(best_model,\n",
    "                                        criterion,\n",
    "                                        img_size,\n",
    "                                        opencv_test_dataloader,\n",
    "                                        device)\n",
    "    print(f\"Best Model - Test Loss: {loss}, Test Accuracy: {accuracy}, Test Sensitivity: {best_sensitivity}\")\n",
    "    #evaluating the last model    \n",
    "    loss, accuracy, last_sensitivity = eval(last_model,\n",
    "                                        criterion,\n",
    "                                        img_size,\n",
    "                                        opencv_test_dataloader,\n",
    "                                        device)\n",
    "    print(f\"Last Model - Test Loss: {loss}, Test Accuracy: {accuracy}, Test Sensitivity: {last_sensitivity}\\n\")\n",
    "\n",
    "    #adding best model to svm_model_list\n",
    "    if best_sensitivity > last_sensitivity:\n",
    "        densenet_svm_model_list.append([best_model, f\"DenseNet_opencv_{num_first_unfreeze}_{num_last_unfreeze}_weighted\", (224,224)])\n",
    "    \n",
    "    else:\n",
    "        densenet_svm_model_list.append([last_model, f\"DenseNet_opencv_{num_first_unfreeze}_{num_last_unfreeze}_weighted\", (224,224)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MaxVit Layer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = calculate_dr_class_weights(opencv_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "layer_list = [[0, 2],\n",
    "              [1, 2],\n",
    "              [0, 8],\n",
    "              [1, 8],\n",
    "              [0, 16],\n",
    "              [1, 16],\n",
    "              [0, 32],\n",
    "              [1, 32]]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 30\n",
    "\n",
    "all_results = pd.DataFrame(columns=[\"Model\", \"train loss\", \"val loss\", \"val accuracy\", \"val sensitivity\"])\n",
    "\n",
    "for num_first_unfreeze, num_last_unfreeze in layer_list:\n",
    "\n",
    "    #clear cuda memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    #reinitialise model with correct weights, loss and optimizer\n",
    "    model = PreTrainedCNNModels(\"MaxVit\", num_first_unfreeze, num_last_unfreeze, len(opencv_train_dataset.classes)).to(device)\n",
    "    criterion=torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(device))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "    print(f\"-------------Training MaxVit with {num_first_unfreeze} Unfrozen First Layers and {num_last_unfreeze} Unfrozen Last Layers------------\")\n",
    "    model_result = train(model,\n",
    "                         criterion,\n",
    "                         optimizer,\n",
    "                         img_size,\n",
    "                         opencv_train_dataloader,\n",
    "                         opencv_val_dataloader,\n",
    "                         'sensitivity',\n",
    "                         num_epochs,\n",
    "                         device,\n",
    "                         f\"MaxVit_opencv_{num_first_unfreeze}_{num_last_unfreeze}_weighted\")\n",
    "    \n",
    "    all_results =  pd.concat([all_results, model_result])\n",
    "\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "layer_list = [[0, 2],\n",
    "              [1, 2],\n",
    "              [0, 8],\n",
    "              [1, 8],\n",
    "              [0, 16],\n",
    "              [1, 16],\n",
    "              [0, 32],\n",
    "              [1, 32]]\n",
    "\n",
    "#Creating densenet SVM model list\n",
    "maxvit_svm_model_list = []\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for num_first_unfreeze, num_last_unfreeze in layer_list:\n",
    "\n",
    "    #clear cuda memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    best_model = torch.load(f'../models/cnn/MaxVit_opencv_{num_first_unfreeze}_{num_last_unfreeze}_weighted_best.pt')\n",
    "    last_model = torch.load(f'../models/cnn/MaxVit_opencv_{num_first_unfreeze}_{num_last_unfreeze}_weighted_last.pt')\n",
    "    criterion=torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    print(f\"-------------Evaluating MaxVit with {num_first_unfreeze} Unfrozen First Layers and {num_last_unfreeze} Unfrozen Last Layers------------\")\n",
    "    #evaluating the best model\n",
    "    loss, accuracy, best_sensitivity = eval(best_model,\n",
    "                                        criterion,\n",
    "                                        img_size,\n",
    "                                        opencv_test_dataloader,\n",
    "                                        device)\n",
    "    print(f\"Best Model - Test Loss: {loss}, Test Accuracy: {accuracy}, Test Sensitivity: {best_sensitivity}\")\n",
    "    #evaluating the last model    \n",
    "    loss, accuracy, last_sensitivity = eval(last_model,\n",
    "                                        criterion,\n",
    "                                        img_size,\n",
    "                                        ben_test_dataloader,\n",
    "                                        device)\n",
    "    print(f\"Last Model - Test Loss: {loss}, Test Accuracy: {accuracy}, Test Sensitivity: {last_sensitivity}\\n\")\n",
    "\n",
    "    #adding best model to svm_model_list\n",
    "    if best_sensitivity > last_sensitivity:\n",
    "        maxvit_svm_model_list.append([best_model, f\"MaxVit_opencv_{num_first_unfreeze}_{num_last_unfreeze}_weighted\", (224,224)])\n",
    "    \n",
    "    else:\n",
    "        maxvit_svm_model_list.append([last_model, f\"MaxVit_opencv_{num_first_unfreeze}_{num_last_unfreeze}_weighted\", (224,224)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "recall_results = train_svm([densenet_svm_model_list, maxvit_svm_model_list], opencv_train_dataloader, opencv_test_dataloader, device, True)\n",
    "\n",
    "recall_results.to_csv(\"../results/densenet_maxvit_opencv.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet and MaxVit Layer Exploration with SVM for UNET_Binary Dataset\n",
    "\n",
    "Exploring different numbers of unfrozen layers in DenseNet and MaxViT with UNET_Binary dataset before passing it through an SVM\n",
    "\n",
    "The layers explored will be\n",
    "- first 0 layer unfrozen and last layer unfrozen (done earlier)\n",
    "- first 1 layer unfrozen and last layers unfrozen\n",
    "- first 0 layer unfrozen and last 8 layers unfrozen (done earlier)\n",
    "- first 1 layer unfrozen and last 8 layers unfrozen\n",
    "- first 0 layer unfrozen and last 16 layers unfrozen\n",
    "- first 1 layer unfrozen and last 16 layers unfrozen\n",
    "- first 0 layer unfrozen and last 32 layers unfrozen\n",
    "- first 0 layer unfrozen and last 32 layers unfrozen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DenseNet Layer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = calculate_dr_class_weights(unetb_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "layer_list = [[0, 2],\n",
    "              [1, 2],\n",
    "              [0, 8],\n",
    "              [1, 8],\n",
    "              [0, 16],\n",
    "              [1, 16],\n",
    "              [0, 32],\n",
    "              [1, 32]]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 30\n",
    "\n",
    "all_results = pd.DataFrame(columns=[\"Model\", \"train loss\", \"val loss\", \"val accuracy\", \"val sensitivity\"])\n",
    "\n",
    "for num_first_unfreeze, num_last_unfreeze in layer_list:\n",
    "\n",
    "    #clear cuda memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    #reinitialise model with correct weights, loss and optimizer\n",
    "    model = PreTrainedCNNModels(\"DenseNet\", num_first_unfreeze, num_last_unfreeze, len(unetb_train_dataset.classes)).to(device)\n",
    "    criterion=torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(device))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "    print(f\"-------------Training DenseNet with {num_first_unfreeze} Unfrozen First Layers and {num_last_unfreeze} Unfrozen Last Layers------------\")\n",
    "    model_result = train(model,\n",
    "                         criterion,\n",
    "                         optimizer,\n",
    "                         img_size,\n",
    "                         unetb_train_dataloader,\n",
    "                         unetb_val_dataloader,\n",
    "                         'sensitivity',\n",
    "                         num_epochs,\n",
    "                         device,\n",
    "                         f\"DenseNet_unetb_{num_first_unfreeze}_{num_last_unfreeze}_weighted\")\n",
    "    \n",
    "    all_results =  pd.concat([all_results, model_result])\n",
    "\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "layer_list = [[0, 2],\n",
    "              [1, 2],\n",
    "              [0, 8],\n",
    "              [1, 8],\n",
    "              [0, 16],\n",
    "              [1, 16],\n",
    "              [0, 32],\n",
    "              [1, 32]]\n",
    "\n",
    "#Creating densenet SVM model list\n",
    "densenet_svm_model_list = []\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for num_first_unfreeze, num_last_unfreeze in layer_list:\n",
    "\n",
    "    #clear cuda memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    best_model = torch.load(f'../models/cnn/DenseNet_unetb_{num_first_unfreeze}_{num_last_unfreeze}_weighted_best.pt')\n",
    "    last_model = torch.load(f'../models/cnn/DenseNet_unetb_{num_first_unfreeze}_{num_last_unfreeze}_weighted_last.pt')\n",
    "    criterion=torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    print(f\"-------------Evaluating DenseNet with {num_first_unfreeze} Unfrozen First Layers and {num_last_unfreeze} Unfrozen Last Layers------------\")\n",
    "    #evaluating the best model\n",
    "    loss, accuracy, best_sensitivity = eval(best_model,\n",
    "                                        criterion,\n",
    "                                        img_size,\n",
    "                                        unetb_test_dataloader,\n",
    "                                        device)\n",
    "    print(f\"Best Model - Test Loss: {loss}, Test Accuracy: {accuracy}, Test Sensitivity: {best_sensitivity}\")\n",
    "    #evaluating the last model    \n",
    "    loss, accuracy, last_sensitivity = eval(last_model,\n",
    "                                        criterion,\n",
    "                                        img_size,\n",
    "                                        unetb_test_dataloader,\n",
    "                                        device)\n",
    "    print(f\"Last Model - Test Loss: {loss}, Test Accuracy: {accuracy}, Test Sensitivity: {last_sensitivity}\\n\")\n",
    "\n",
    "    #adding best model to svm_model_list\n",
    "    if best_sensitivity > last_sensitivity:\n",
    "        densenet_svm_model_list.append([best_model, f\"DenseNet_unetb_{num_first_unfreeze}_{num_last_unfreeze}_weighted\", (224,224)])\n",
    "    \n",
    "    else:\n",
    "        densenet_svm_model_list.append([last_model, f\"DenseNet_unetb_{num_first_unfreeze}_{num_last_unfreeze}_weighted\", (224,224)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MaxVit Layer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = calculate_dr_class_weights(unetb_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "layer_list = [[0, 2],\n",
    "              [1, 2],\n",
    "              [0, 8],\n",
    "              [1, 8],\n",
    "              [0, 16],\n",
    "              [1, 16],\n",
    "              [0, 32],\n",
    "              [1, 32]]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 30\n",
    "\n",
    "all_results = pd.DataFrame(columns=[\"Model\", \"train loss\", \"val loss\", \"val accuracy\", \"val sensitivity\"])\n",
    "\n",
    "for num_first_unfreeze, num_last_unfreeze in layer_list:\n",
    "\n",
    "    #clear cuda memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    #reinitialise model with correct weights, loss and optimizer\n",
    "    model = PreTrainedCNNModels(\"MaxVit\", num_first_unfreeze, num_last_unfreeze, len(unetb_train_dataset.classes)).to(device)\n",
    "    criterion=torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(device))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "    print(f\"-------------Training MaxVit with {num_first_unfreeze} Unfrozen First Layers and {num_last_unfreeze} Unfrozen Last Layers------------\")\n",
    "    model_result = train(model,\n",
    "                         criterion,\n",
    "                         optimizer,\n",
    "                         img_size,\n",
    "                         unetb_train_dataloader,\n",
    "                         unetb_val_dataloader,\n",
    "                         'sensitivity',\n",
    "                         num_epochs,\n",
    "                         device,\n",
    "                         f\"MaxVit_unetb_{num_first_unfreeze}_{num_last_unfreeze}_weighted\")\n",
    "    \n",
    "    all_results =  pd.concat([all_results, model_result])\n",
    "\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "layer_list = [[0, 2],\n",
    "              [1, 2],\n",
    "              [0, 8],\n",
    "              [1, 8],\n",
    "              [0, 16],\n",
    "              [1, 16],\n",
    "              [0, 32],\n",
    "              [1, 32]]\n",
    "\n",
    "#Creating densenet SVM model list\n",
    "maxvit_svm_model_list = []\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for num_first_unfreeze, num_last_unfreeze in layer_list:\n",
    "\n",
    "    #clear cuda memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    best_model = torch.load(f'../models/cnn/MaxVit_unetb_{num_first_unfreeze}_{num_last_unfreeze}_weighted_best.pt')\n",
    "    last_model = torch.load(f'../models/cnn/MaxVit_unetb_{num_first_unfreeze}_{num_last_unfreeze}_weighted_last.pt')\n",
    "    criterion=torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    print(f\"-------------Evaluating DenseNet with {num_first_unfreeze} Unfrozen First Layers and {num_last_unfreeze} Unfrozen Last Layers------------\")\n",
    "    #evaluating the best model\n",
    "    loss, accuracy, best_sensitivity = eval(best_model,\n",
    "                                        criterion,\n",
    "                                        img_size,\n",
    "                                        unetb_test_dataloader,\n",
    "                                        device)\n",
    "    print(f\"Best Model - Test Loss: {loss}, Test Accuracy: {accuracy}, Test Sensitivity: {best_sensitivity}\")\n",
    "    #evaluating the last model    \n",
    "    loss, accuracy, last_sensitivity = eval(last_model,\n",
    "                                        criterion,\n",
    "                                        img_size,\n",
    "                                        unetb_test_dataloader,\n",
    "                                        device)\n",
    "    print(f\"Last Model - Test Loss: {loss}, Test Accuracy: {accuracy}, Test Sensitivity: {last_sensitivity}\\n\")\n",
    "\n",
    "    #adding best model to svm_model_list\n",
    "    if best_sensitivity > last_sensitivity:\n",
    "        maxvit_svm_model_list.append([best_model, f\"MaxVit_unetb_{num_first_unfreeze}_{num_last_unfreeze}_weighted\", (224,224)])\n",
    "    \n",
    "    else:\n",
    "        maxvit_svm_model_list.append([last_model, f\"MaxVit_unetb_{num_first_unfreeze}_{num_last_unfreeze}_weighted\", (224,224)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "recall_results = train_svm([densenet_svm_model_list, maxvit_svm_model_list], unetb_train_dataloader, unetb_test_dataloader, device, True)\n",
    "\n",
    "recall_results.to_csv(\"../results/densenet_maxvit_unetb.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet and MaxVit Layer Exploration with SVM for UNET_Multiclass Dataset\n",
    "\n",
    "Exploring different numbers of unfrozen layers in DenseNet and MaxViT with UNET_Multiclass dataset before passing it through an SVM\n",
    "\n",
    "The layers explored will be\n",
    "- first 0 layer unfrozen and last layer unfrozen (done earlier)\n",
    "- first 1 layer unfrozen and last layers unfrozen\n",
    "- first 0 layer unfrozen and last 8 layers unfrozen (done earlier)\n",
    "- first 1 layer unfrozen and last 8 layers unfrozen\n",
    "- first 0 layer unfrozen and last 16 layers unfrozen\n",
    "- first 1 layer unfrozen and last 16 layers unfrozen\n",
    "- first 0 layer unfrozen and last 32 layers unfrozen\n",
    "- first 0 layer unfrozen and last 32 layers unfrozen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DenseNet Layer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = calculate_dr_class_weights(unetm_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "layer_list = [[0, 2],\n",
    "              [1, 2],\n",
    "              [0, 8],\n",
    "              [1, 8],\n",
    "              [0, 16],\n",
    "              [1, 16],\n",
    "              [0, 32],\n",
    "              [1, 32]]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 30\n",
    "\n",
    "all_results = pd.DataFrame(columns=[\"Model\", \"train loss\", \"val loss\", \"val accuracy\", \"val sensitivity\"])\n",
    "\n",
    "for num_first_unfreeze, num_last_unfreeze in layer_list:\n",
    "\n",
    "    #clear cuda memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    #reinitialise model with correct weights, loss and optimizer\n",
    "    model = PreTrainedCNNModels(\"DenseNet\", num_first_unfreeze, num_last_unfreeze, len(unetm_train_dataset.classes)).to(device)\n",
    "    criterion=torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(device))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "    print(f\"-------------Training DenseNet with {num_first_unfreeze} Unfrozen First Layers and {num_last_unfreeze} Unfrozen Last Layers------------\")\n",
    "    model_result = train(model,\n",
    "                         criterion,\n",
    "                         optimizer,\n",
    "                         img_size,\n",
    "                         unetm_train_dataloader,\n",
    "                         unetm_val_dataloader,\n",
    "                         'sensitivity',\n",
    "                         num_epochs,\n",
    "                         device,\n",
    "                         f\"DenseNet_unetm_{num_first_unfreeze}_{num_last_unfreeze}_weighted\")\n",
    "    \n",
    "    all_results =  pd.concat([all_results, model_result])\n",
    "\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "layer_list = [[0, 2],\n",
    "              [1, 2],\n",
    "              [0, 8],\n",
    "              [1, 8],\n",
    "              [0, 16],\n",
    "              [1, 16],\n",
    "              [0, 32],\n",
    "              [1, 32]]\n",
    "\n",
    "#Creating densenet SVM model list\n",
    "densenet_svm_model_list = []\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for num_first_unfreeze, num_last_unfreeze in layer_list:\n",
    "\n",
    "    #clear cuda memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    best_model = torch.load(f'../models/cnn/DenseNet_unetm_{num_first_unfreeze}_{num_last_unfreeze}_weighted_best.pt')\n",
    "    last_model = torch.load(f'../models/cnn/DenseNet_unetm_{num_first_unfreeze}_{num_last_unfreeze}_weighted_last.pt')\n",
    "    criterion=torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    print(f\"-------------Evaluating DenseNet with {num_first_unfreeze} Unfrozen First Layers and {num_last_unfreeze} Unfrozen Last Layers------------\")\n",
    "    #evaluating the best model\n",
    "    loss, accuracy, best_sensitivity = eval(best_model,\n",
    "                                        criterion,\n",
    "                                        img_size,\n",
    "                                        unetm_test_dataloader,\n",
    "                                        device)\n",
    "    print(f\"Best Model - Test Loss: {loss}, Test Accuracy: {accuracy}, Test Sensitivity: {best_sensitivity}\")\n",
    "    #evaluating the last model    \n",
    "    loss, accuracy, last_sensitivity = eval(last_model,\n",
    "                                        criterion,\n",
    "                                        img_size,\n",
    "                                        unetm_test_dataloader,\n",
    "                                        device)\n",
    "    print(f\"Last Model - Test Loss: {loss}, Test Accuracy: {accuracy}, Test Sensitivity: {last_sensitivity}\\n\")\n",
    "\n",
    "    #adding best model to svm_model_list\n",
    "    if best_sensitivity > last_sensitivity:\n",
    "        densenet_svm_model_list.append([best_model, f\"DenseNet_unetm_{num_first_unfreeze}_{num_last_unfreeze}_weighted\", (224,224)])\n",
    "    \n",
    "    else:\n",
    "        densenet_svm_model_list.append([last_model, f\"DenseNet_unetm_{num_first_unfreeze}_{num_last_unfreeze}_weighted\", (224,224)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MaxVit Layer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = calculate_dr_class_weights(unetm_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "layer_list = [[0, 2],\n",
    "              [1, 2],\n",
    "              [0, 8],\n",
    "              [1, 8],\n",
    "              [0, 16],\n",
    "              [1, 16],\n",
    "              [0, 32],\n",
    "              [1, 32]]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 30\n",
    "\n",
    "all_results = pd.DataFrame(columns=[\"Model\", \"train loss\", \"val loss\", \"val accuracy\", \"val sensitivity\"])\n",
    "\n",
    "for num_first_unfreeze, num_last_unfreeze in layer_list:\n",
    "\n",
    "    #clear cuda memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    #reinitialise model with correct weights, loss and optimizer\n",
    "    model = PreTrainedCNNModels(\"MaxVit\", num_first_unfreeze, num_last_unfreeze, len(unetm_train_dataset.classes)).to(device)\n",
    "    criterion=torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(device))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "    print(f\"-------------Training MaxVit with {num_first_unfreeze} Unfrozen First Layers and {num_last_unfreeze} Unfrozen Last Layers------------\")\n",
    "    model_result = train(model,\n",
    "                         criterion,\n",
    "                         optimizer,\n",
    "                         img_size,\n",
    "                         unetm_train_dataloader,\n",
    "                         unetm_val_dataloader,\n",
    "                         'sensitivity',\n",
    "                         num_epochs,\n",
    "                         device,\n",
    "                         f\"MaxVit_unetm_{num_first_unfreeze}_{num_last_unfreeze}_weighted\")\n",
    "    \n",
    "    all_results =  pd.concat([all_results, model_result])\n",
    "\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "layer_list = [[0, 2],\n",
    "              [1, 2],\n",
    "              [0, 8],\n",
    "              [1, 8],\n",
    "              [0, 16],\n",
    "              [1, 16],\n",
    "              [0, 32],\n",
    "              [1, 32]]\n",
    "\n",
    "#Creating densenet SVM model list\n",
    "maxvit_svm_model_list = []\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for num_first_unfreeze, num_last_unfreeze in layer_list:\n",
    "\n",
    "    #clear cuda memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    best_model = torch.load(f'../models/cnn/MaxVit_unetm_{num_first_unfreeze}_{num_last_unfreeze}_weighted_best.pt')\n",
    "    last_model = torch.load(f'../models/cnn/MaxVit_unetm_{num_first_unfreeze}_{num_last_unfreeze}_weighted_last.pt')\n",
    "    criterion=torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    print(f\"-------------Evaluating DenseNet with {num_first_unfreeze} Unfrozen First Layers and {num_last_unfreeze} Unfrozen Last Layers------------\")\n",
    "    #evaluating the best model\n",
    "    loss, accuracy, best_sensitivity = eval(best_model,\n",
    "                                        criterion,\n",
    "                                        img_size,\n",
    "                                        unetm_test_dataloader,\n",
    "                                        device)\n",
    "    print(f\"Best Model - Test Loss: {loss}, Test Accuracy: {accuracy}, Test Sensitivity: {best_sensitivity}\")\n",
    "    #evaluating the last model    \n",
    "    loss, accuracy, last_sensitivity = eval(last_model,\n",
    "                                        criterion,\n",
    "                                        img_size,\n",
    "                                        unetm_test_dataloader,\n",
    "                                        device)\n",
    "    print(f\"Last Model - Test Loss: {loss}, Test Accuracy: {accuracy}, Test Sensitivity: {last_sensitivity}\\n\")\n",
    "\n",
    "    #adding best model to svm_model_list\n",
    "    if best_sensitivity > last_sensitivity:\n",
    "        maxvit_svm_model_list.append([best_model, f\"MaxVit_unetm_{num_first_unfreeze}_{num_last_unfreeze}_weighted\", (224,224)])\n",
    "    \n",
    "    else:\n",
    "        maxvit_svm_model_list.append([last_model, f\"MaxVit_unetm_{num_first_unfreeze}_{num_last_unfreeze}_weighted\", (224,224)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "recall_results = train_svm([densenet_svm_model_list, maxvit_svm_model_list], unetm_train_dataloader, unetm_test_dataloader, device, True)\n",
    "\n",
    "recall_results.to_csv(\"../results/densenet_maxvit_unetm.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
